# Document Portal - Cost & Performance Optimization Analysis

## Executive Summary

**Current State**: The Document Portal processes documents effectively but has significant opportunities for reducing:
- **LLM API costs** by 60-80%
- **Computational time** by 50-70%
- **Resource usage** by 40-60%

**Potential Savings**:
- **Cost**: $150-200/year â†’ $30-60/year (at 1000 documents/day)
- **Processing Time**: 5-8 seconds/document â†’ 1-3 seconds/document
- **CPU Usage**: 70-90% â†’ 20-40% during peak load

---

## 1. LLM Usage Analysis (HIGHEST COST IMPACT)

### Current LLM Costs

| Component | Current Behavior | Cost per Call | Daily Volume (est) | Monthly Cost |
|-----------|------------------|---------------|---------------------|--------------|
| ID Extraction (Gemini Vision) | Called when confidence < 80% or name/address missing | $0.0005 | 200 IDs | $30 |
| Invoice Extraction (Gemini Vision) | **Always called** by default | $0.0005 | 500 invoices | $75 |
| **Total** | | | | **$105/month** |

*Based on Gemini 2.0 Flash pricing: ~$0.0005 per image*

### Problem Areas

#### 1.1 Invoice Extraction - Unconditional LLM Usage
**Location**: `api/main.py:464-524`

```python
@app.post("/extract/invoice")
async def extract_invoice_endpoint(
    files: List[UploadFile] = File(...),
    use_gemini: bool = True  # âŒ ALWAYS uses expensive LLM
):
    if use_gemini:
        result = gemini_extractor.extract_data(str(temp_path))  # $$$
```

**Problem**:
- **Always calls Gemini Vision** even for simple invoices
- Regex extraction (`invoice_extractor`) is never tried first
- Simple invoices with clear text waste API calls

**Impact**:
- ~70% of invoices could be handled with regex alone
- **Wasting**: 350 LLM calls/day Ã— $0.0005 = **$5.25/day = $157/month**

#### 1.2 ID Extraction - Inefficient Fallback Strategy
**Location**: `extractor.py:247-334`

```python
def extract_id_data(self, text: str = None, image_path: str = None,
                   use_vision_first: bool = False):
    # Try regex
    result = self.extract_from_text(text)

    # âŒ Falls back to Gemini Vision too eagerly
    if result["confidence"] < 80 or missing_critical:
        vision_result = self.extract_from_image_vision(image_path)  # $$$
```

**Problem**:
- Falls back to Gemini when regex confidence < 80%
- Threshold too conservative (70% would work fine)
- No attempt to improve OCR before calling LLM

**Impact**:
- ~30% of IDs trigger unnecessary Gemini calls
- **Wasting**: 60 LLM calls/day Ã— $0.0005 = **$0.90/day = $27/month**

### Optimization Strategy - Intelligent LLM Usage

#### Strategy 1: Try Regex First, LLM Last
```python
# Invoice Processing Flow
1. Try regex extraction (FREE, 0.1s)
   â†“ (if confidence < 70%)
2. Try enhanced OCR with preprocessing (FREE, 0.5s)
   â†“ (if confidence < 70%)
3. Use Gemini Vision ($$$, 1.5s)

# Expected Savings: 70% fewer Gemini calls
```

#### Strategy 2: Confidence Threshold Tuning
```python
# Current: Confidence < 80% â†’ Gemini
# Optimized: Confidence < 60% â†’ Gemini

# For most documents, 60% confidence is acceptable
# Save 20-30% on Gemini calls
```

#### Strategy 3: Batch Processing (Future Enhancement)
```python
# Instead of: 10 images Ã— 10 API calls = $0.005
# Use: 1 batch call with 10 images = $0.0015 (70% savings)
```

**Total LLM Cost Savings**: $184/month â†’ $40/month = **$144/month saved (78% reduction)**

---

## 2. Computational Bottlenecks

### Current Processing Pipeline

```
Document Upload (0.1s)
    â†“
Scanner Preprocessing (2-3s) â† ðŸ”´ BOTTLENECK
    - Shadow removal (0.8s)
    - Rotation detection (0.5s)
    - Blur detection (0.2s)
    - Sharpening (0.3s)
    - Edge enhancement (0.4s)
    - Contour detection (0.5s)
    â†“
Compression with SSIM (2-5s) â† ðŸ”´ BOTTLENECK
    - Binary search: 8-10 iterations
    - Each: save + load + calculate (0.3s)
    â†“
OCR Processing (1-2s)
    - Tesseract (1.5s)
    â†“
Extraction (regex: 0.1s | Gemini: 1.5s)
    â†“
Total: 5-12 seconds per document
```

### Problem Areas

#### 2.1 Scanner Always Runs All Steps
**Location**: `scanner.py:129-228`

```python
def scan_document(self, image_path: str, output_path: str = None, enhance: bool = True):
    # âŒ ALWAYS runs all preprocessing steps
    rotation_angle = self._detect_rotation_angle(img)  # 0.5s
    img = self._remove_shadow(img)  # 0.8s
    blur_var, is_blurry = self._detect_blur(img)  # 0.2s
    if is_blurry:
        img = self._sharpen_image(img)  # 0.3s
    enhanced_gray = self._enhance_document_edges(img)  # 0.4s
    # ... more processing
```

**Problem**:
- No quality check before preprocessing
- Runs shadow removal on well-lit images
- Runs sharpening on already-sharp images
- No fast path for high-quality inputs

**Impact**:
- **Wastes 1-2 seconds** on 60% of images that don't need enhancement
- Unnecessary CPU usage

#### 2.2 Compression Runs Binary Search Every Time
**Location**: `ingestion.py:82-232`

```python
def compress_with_target_quality(self, image_path: Path, target_ssim: float = 0.98):
    # âŒ Binary search: 8-10 iterations
    while low <= high:
        mid_quality = (low + high) // 2
        # Save, load, calculate SSIM (0.3s per iteration)
        buffer = io.BytesIO()
        img.save(buffer, "JPEG", quality=mid_quality)
        compressed_img = Image.open(buffer)
        current_ssim = ssim(original_array, compressed_array)
```

**Problem**:
- Always runs SSIM binary search (8-10 iterations)
- Each iteration: save + load + calculate (0.3s) = **2.4-3s total**
- No check if image is already compressed
- No fast estimation

**Impact**:
- **Wastes 2-3 seconds** per image
- Runs on images that may already be compressed

#### 2.3 OCR Has No Caching
**Location**: `ingestion.py:234-252`

```python
def _process_image(self, image_path: Path) -> str:
    # âŒ No caching - always runs Tesseract
    self.compress_image(image_path)  # 0.5s
    img = cv2.imread(str(image_path))
    text = pytesseract.image_to_string(img)  # 1.5s
    return text
```

**Problem**:
- No caching of OCR results
- Duplicate uploads re-run OCR
- No image hash checking

**Impact**:
- **Wastes 2 seconds** on duplicate documents (10-20% of uploads)

### Optimization Strategy - Selective Processing

#### Strategy 1: Quick Quality Check
```python
def quick_quality_check(img: np.ndarray) -> dict:
    """
    Fast quality assessment (0.05s).
    Returns: {"needs_shadow_removal": bool, "needs_rotation": bool, ...}
    """
    # Check brightness variance (shadow indicator)
    brightness_var = np.var(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY))
    needs_shadow = brightness_var < 1000

    # Check edge sharpness (blur indicator)
    blur_score = cv2.Laplacian(gray, cv2.CV_64F).var()
    needs_sharpen = blur_score < 100

    # Check aspect ratio (rotation indicator)
    # ... more checks

    return {
        "needs_shadow_removal": needs_shadow,
        "needs_sharpening": needs_sharpen,
        "needs_rotation": False,  # Based on quick check
        "is_high_quality": not (needs_shadow or needs_sharpen)
    }

# Then only run needed preprocessing steps
if quality["needs_shadow_removal"]:
    img = self._remove_shadow(img)
if quality["needs_sharpening"]:
    img = self._sharpen_image(img)
```

**Savings**: 50-70% reduction in preprocessing time

#### Strategy 2: Fast Compression Path
```python
def compress_image_smart(self, image_path: Path):
    """
    Smart compression with fast path.
    """
    file_size = image_path.stat().st_size

    # Fast path: already compressed
    if file_size < 500_000:  # < 500KB
        return image_path  # Skip compression

    # Fast path: estimate quality from file size
    estimated_quality = self._estimate_quality_from_size(file_size)

    # Only run SSIM binary search if critical document
    if is_critical_document:
        return self.compress_with_target_quality(image_path)
    else:
        # Use estimated quality (no SSIM calculation)
        return self.compress_image(image_path, quality=estimated_quality)
```

**Savings**: 60-80% reduction in compression time

#### Strategy 3: OCR Result Caching
```python
import hashlib

class OCRCache:
    def __init__(self):
        self.cache = {}  # or Redis for distributed

    def get_image_hash(self, image_path: Path) -> str:
        """Fast perceptual hash of image."""
        with Image.open(image_path) as img:
            # Resize to 8x8 for perceptual hash
            img = img.resize((8, 8), Image.LANCZOS).convert('L')
            pixels = list(img.getdata())
            avg = sum(pixels) / len(pixels)
            bits = ''.join('1' if p > avg else '0' for p in pixels)
            return hashlib.md5(bits.encode()).hexdigest()

    def get_ocr_result(self, image_hash: str) -> Optional[str]:
        """Get cached OCR result."""
        return self.cache.get(image_hash)

    def save_ocr_result(self, image_hash: str, text: str):
        """Cache OCR result."""
        self.cache[image_hash] = text

# Usage
image_hash = ocr_cache.get_image_hash(image_path)
cached_text = ocr_cache.get_ocr_result(image_hash)
if cached_text:
    return cached_text  # âœ… Instant (0.01s)
else:
    text = pytesseract.image_to_string(img)
    ocr_cache.save_ocr_result(image_hash, text)
    return text
```

**Savings**: 90% faster for duplicate documents (10-20% of uploads)

**Total Computational Time Savings**: 5-8 seconds â†’ 1.5-3 seconds = **50-70% reduction**

---

## 3. Caching Strategy

### Current Caching (Minimal)

| What's Cached | Storage | Hit Rate | Benefit |
|---------------|---------|----------|---------|
| ID extraction results (by user_id) | JSON file | ~30% | Medium |

**Problems**:
- Only caches final ID results
- No OCR text caching
- No Gemini Vision caching
- No preprocessed image caching
- No invoice extraction caching
- File-based (slow for distributed systems)

### Proposed Caching Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CACHING LAYERS                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  L1: In-Memory Cache (LRU, 100MB)                      â”‚
â”‚      - Recent OCR results                               â”‚
â”‚      - Recent extractions                               â”‚
â”‚      - TTL: 1 hour                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  L2: Redis Cache (Distributed, 1GB)                    â”‚
â”‚      - OCR results by image hash                        â”‚
â”‚      - Gemini Vision results by image hash              â”‚
â”‚      - Extraction results by user_id                    â”‚
â”‚      - TTL: 7 days                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  L3: Persistent Storage (S3/Disk)                      â”‚
â”‚      - Preprocessed images                              â”‚
â”‚      - Historical extractions                           â”‚
â”‚      - TTL: 30 days                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Implementation: Multi-Layer Cache

```python
from functools import lru_cache
import redis
import hashlib
from typing import Optional

class DocumentCache:
    def __init__(self):
        # L1: In-memory LRU cache (fast, 100MB)
        self.memory_cache = {}

        # L2: Redis (distributed, persistent)
        try:
            self.redis = redis.Redis(host='localhost', port=6379, db=0)
        except:
            self.redis = None

    def get_image_hash(self, image_path: Path) -> str:
        """Generate perceptual hash of image."""
        # Fast hash based on image content (not file path)
        with Image.open(image_path) as img:
            img_small = img.resize((8, 8), Image.LANCZOS).convert('L')
            pixels = list(img_small.getdata())
            avg = sum(pixels) / len(pixels)
            bits = ''.join('1' if p > avg else '0' for p in pixels)
            return hashlib.md5(bits.encode()).hexdigest()

    def get_ocr_text(self, image_hash: str) -> Optional[str]:
        """Get cached OCR text (L1 â†’ L2)."""
        # Try L1 (memory)
        if image_hash in self.memory_cache:
            return self.memory_cache[image_hash]['ocr']

        # Try L2 (Redis)
        if self.redis:
            cached = self.redis.get(f"ocr:{image_hash}")
            if cached:
                self.memory_cache[image_hash] = {'ocr': cached.decode()}
                return cached.decode()

        return None

    def save_ocr_text(self, image_hash: str, text: str):
        """Cache OCR text (L1 + L2)."""
        # Save to L1
        self.memory_cache[image_hash] = {'ocr': text}

        # Save to L2 (7 day TTL)
        if self.redis:
            self.redis.setex(f"ocr:{image_hash}", 604800, text)

    def get_gemini_result(self, image_hash: str) -> Optional[dict]:
        """Get cached Gemini Vision result."""
        if self.redis:
            cached = self.redis.get(f"gemini:{image_hash}")
            if cached:
                import json
                return json.loads(cached.decode())
        return None

    def save_gemini_result(self, image_hash: str, result: dict):
        """Cache Gemini Vision result (expensive API call)."""
        if self.redis:
            import json
            # 30 day TTL for expensive LLM calls
            self.redis.setex(f"gemini:{image_hash}", 2592000, json.dumps(result))

# Singleton
DOCUMENT_CACHE = DocumentCache()
```

#### Usage in Endpoints

```python
# In api/main.py - ID Extraction
@app.post("/extract/id")
async def extract_id_endpoint(file: UploadFile = File(...)):
    # 1. Save file
    temp_path = save_uploaded_file(file)

    # 2. Check cache by image hash
    image_hash = DOCUMENT_CACHE.get_image_hash(temp_path)

    # 3. Try to get cached OCR
    cached_ocr = DOCUMENT_CACHE.get_ocr_text(image_hash)
    if cached_ocr:
        text = cached_ocr  # âœ… 100x faster
    else:
        text = ingestion._process_image(temp_path)
        DOCUMENT_CACHE.save_ocr_text(image_hash, text)

    # 4. Try regex extraction
    result = id_extractor.extract_from_text(text)

    # 5. Check if Gemini Vision needed
    if result["confidence"] < 60:
        # Try cached Gemini result
        cached_gemini = DOCUMENT_CACHE.get_gemini_result(image_hash)
        if cached_gemini:
            result = cached_gemini  # âœ… No API call
        else:
            result = id_extractor.extract_from_image_vision(temp_path)
            DOCUMENT_CACHE.save_gemini_result(image_hash, result)

    return {"extracted": result}
```

**Cache Hit Rate Expectations**:
- OCR cache: 15-25% hit rate (duplicate uploads)
- Gemini cache: 10-20% hit rate
- **Savings**: 15-25% faster processing, 10-20% fewer LLM calls

---

## 4. OCR Optimization

### Current OCR Performance

```python
def _process_image(self, image_path: Path) -> str:
    self.compress_image(image_path)  # 0.5s
    img = cv2.imread(str(image_path))
    text = pytesseract.image_to_string(img)  # 1.5s â† SLOW
    return text
```

**Problems**:
- Uses default Tesseract PSM mode (fully automatic)
- No optimization for document type
- No parallel processing for multi-page documents

### Optimization: Tesseract PSM Modes

```python
def _process_image_optimized(self, image_path: Path, doc_type: str = "auto") -> str:
    """
    Optimized OCR with PSM mode selection.

    PSM Modes:
    - PSM 3: Fully automatic (default) - SLOW
    - PSM 6: Single uniform block - FAST for single-column documents
    - PSM 11: Sparse text - FAST for IDs/receipts
    - PSM 4: Single column - FAST for invoices
    """

    # Select PSM based on document type
    psm_map = {
        "id": 11,        # Sparse text (driver's license)
        "invoice": 4,    # Single column
        "receipt": 6,    # Uniform block
        "contract": 3,   # Fully automatic
        "auto": 3
    }

    psm = psm_map.get(doc_type, 3)

    # Optimize Tesseract config
    custom_config = f'--psm {psm} --oem 3'  # OEM 3 = LSTM only (faster)

    img = cv2.imread(str(image_path))
    text = pytesseract.image_to_string(img, config=custom_config)

    return text
```

**Savings**: 30-50% faster OCR for known document types

### Optimization: Parallel Multi-Page Processing

```python
from concurrent.futures import ThreadPoolExecutor

def _process_pdf_parallel(self, pdf_path: Path) -> str:
    """
    Process multi-page PDFs in parallel.
    """
    images = convert_from_path(str(pdf_path))

    # Process pages in parallel (4 workers)
    with ThreadPoolExecutor(max_workers=4) as executor:
        texts = list(executor.map(
            lambda img: pytesseract.image_to_string(img),
            images
        ))

    return "\n".join(texts)
```

**Savings**: 3-4x faster for multi-page documents

---

## 5. Implementation Priority

### Phase 1: Quick Wins (Week 1) - **Highest ROI**

| Optimization | Effort | Impact | Savings |
|-------------|--------|--------|---------|
| 1. Invoice regex-first strategy | Low | High | $100/month |
| 2. ID extraction confidence threshold (80â†’60%) | Low | Medium | $20/month |
| 3. Skip compression for small files | Low | Medium | 1-2s per doc |
| 4. OCR PSM mode optimization | Low | Medium | 0.5s per doc |

**Total Phase 1 Savings**: $120/month + 30% faster

### Phase 2: Caching Infrastructure (Week 2-3)

| Optimization | Effort | Impact | Savings |
|-------------|--------|--------|---------|
| 5. Implement Redis cache | Medium | High | 20% faster |
| 6. OCR result caching by image hash | Medium | High | 15-25% hit rate |
| 7. Gemini result caching | Low | High | $30/month |

**Total Phase 2 Savings**: $30/month + 20-30% faster

### Phase 3: Advanced Optimizations (Week 4-6)

| Optimization | Effort | Impact | Savings |
|-------------|--------|--------|---------|
| 8. Quick quality check before preprocessing | Medium | High | 1-2s per doc |
| 9. Parallel multi-page OCR | Medium | Medium | 3x faster PDFs |
| 10. Batch Gemini Vision processing | High | Medium | 50% LLM cost |

**Total Phase 3 Savings**: $50/month + 40% faster

---

## 6. Cost-Benefit Analysis

### Before Optimization

| Metric | Value |
|--------|-------|
| LLM Cost (monthly) | $105 |
| Avg Processing Time | 6 seconds |
| CPU Usage (peak) | 80% |
| Cache Hit Rate | 30% (IDs only) |

### After Full Optimization

| Metric | Value | Improvement |
|--------|-------|-------------|
| LLM Cost (monthly) | $25 | **76% reduction** |
| Avg Processing Time | 2 seconds | **67% faster** |
| CPU Usage (peak) | 35% | **56% reduction** |
| Cache Hit Rate | 60% (all documents) | **2x improvement** |

### ROI Calculation

**Development Time**: 3-4 weeks (1 developer)
**Annual Savings**:
- LLM costs: $960/year
- Server costs (reduced CPU): $500/year
- **Total: $1,460/year**

**Developer cost**: 4 weeks Ã— $2000/week = $8,000
**Payback period**: 5.5 months

**3-year ROI**: $4,380 savings - $8,000 cost = **-$3,620** (break-even at scale)

*Note: ROI improves dramatically with scale (10x documents = $14,600/year savings)*

---

## 7. Recommended Implementation Plan

### Immediate Actions (This Week)

```python
# 1. Change invoice endpoint default
@app.post("/extract/invoice")
async def extract_invoice_endpoint(
    files: List[UploadFile] = File(...),
    use_gemini: bool = False  # âœ… Changed from True
):
    # Try regex first
    text = ingestion._process_image(temp_path)
    result = invoice_extractor.extract_invoice_data(text)

    # Only use Gemini if confidence < 60%
    if result["confidence"] < 60 and use_gemini:
        result = gemini_extractor.extract_data(str(temp_path))
```

**Instant savings**: $75/month â†’ $15/month = **$60/month saved**

### Week 1-2: Caching Layer

1. Add Redis to `docker-compose.yml`
2. Implement `DocumentCache` class
3. Add caching to ID and invoice endpoints
4. Deploy and monitor cache hit rates

### Week 3-4: Preprocessing Optimization

1. Implement `quick_quality_check()`
2. Add selective preprocessing to scanner
3. Implement smart compression path
4. Add OCR PSM mode selection

### Week 5-6: Advanced Features

1. Batch Gemini processing
2. Parallel multi-page OCR
3. Performance monitoring dashboard
4. A/B testing for thresholds

---

## 8. Monitoring & Metrics

### Key Metrics to Track

```python
# Add to MONITORING_GUIDE.md

# LLM Usage Metrics
llm_calls_total = Counter('llm_calls_total', 'Total LLM API calls', ['endpoint', 'model'])
llm_cost_estimate = Counter('llm_cost_estimate_dollars', 'Estimated LLM costs')
llm_cache_hits = Counter('llm_cache_hits_total', 'LLM cache hits')

# Processing Time Metrics
preprocessing_duration = Histogram('preprocessing_duration_seconds', 'Preprocessing time')
ocr_duration = Histogram('ocr_duration_seconds', 'OCR processing time')
compression_duration = Histogram('compression_duration_seconds', 'Compression time')

# Cache Metrics
cache_hit_ratio = Gauge('cache_hit_ratio', 'Cache hit ratio', ['cache_type'])
cache_size_bytes = Gauge('cache_size_bytes', 'Cache size in bytes', ['cache_type'])
```

### Alert Thresholds

```yaml
- alert: HighLLMUsage
  expr: rate(llm_calls_total[1h]) > 100
  annotations:
    summary: "Unusually high LLM usage detected"
    action: "Check if regex extraction is failing"

- alert: LowCacheHitRate
  expr: cache_hit_ratio < 0.10
  annotations:
    summary: "Cache hit rate below 10%"
    action: "Investigate cache eviction or unique documents"
```

---

## 9. Next Steps

### Action Items

- [ ] Review and approve optimization plan
- [ ] Set up Redis cache infrastructure
- [ ] Implement Phase 1 quick wins
- [ ] Deploy and monitor for 1 week
- [ ] Measure actual savings vs. estimates
- [ ] Proceed to Phase 2 if ROI validates

### Questions to Consider

1. **Scale**: What's the expected document volume growth?
2. **Budget**: What's the acceptable LLM cost per month?
3. **Latency**: What's the target processing time per document?
4. **Accuracy**: What's the minimum acceptable confidence threshold?

---

**Document Portal v2.1.0 - Optimization Roadmap**
*Cost-Effective, Fast, and Scalable Document Processing*
