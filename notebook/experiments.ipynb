{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2656738c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all good\n"
     ]
    }
   ],
   "source": [
    "print(\"all good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c179c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4199a895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REDACTED_GROQ_KEY]\n\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()            # will read .env in cwd\n",
    "import os\n",
    "print(os.getenv(\"GROQ_API_KEY\"))   # verify it loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81d6ad9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8c673f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-20b\", api_key=os.getenv(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bc27c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"What is the capital of France?\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eab3fcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a8addbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c709d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04257791489362717,\n",
       " -0.047810737043619156,\n",
       " -0.02702580951154232,\n",
       " -0.035097863525152206,\n",
       " 0.05324113741517067,\n",
       " 0.0018493696115911007,\n",
       " 0.004823467694222927,\n",
       " -0.022051338106393814,\n",
       " 0.0009697225177660584,\n",
       " 0.07324519753456116]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vector=embedding_model.embed_query(\"What is the capital of France?\")\n",
    "doc_vector[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759d2094",
   "metadata": {},
   "source": [
    "1. Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "538f6b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f314852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1afc6382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2a5e4c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/currycreations/Desktop/New Folder/Document_Portal/notebook'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9d683d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf\"\n",
    "loader = PyPDFLoader(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26ac3eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9451c035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'iOS Version 26.1 (Build 23B85) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20251223025637Z00'00'\", 'title': 'CamScanner 12-22-25 20.52', 'author': 'CamScanner', 'moddate': \"D:20251223025637Z00'00'\", 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 26.1 (Build 23B85) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20251223025637Z00'00'\", 'title': 'CamScanner 12-22-25 20.52', 'author': 'CamScanner', 'moddate': \"D:20251223025637Z00'00'\", 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 26.1 (Build 23B85) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20251223025637Z00'00'\", 'title': 'CamScanner 12-22-25 20.52', 'author': 'CamScanner', 'moddate': \"D:20251223025637Z00'00'\", 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 26.1 (Build 23B85) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20251223025637Z00'00'\", 'title': 'CamScanner 12-22-25 20.52', 'author': 'CamScanner', 'moddate': \"D:20251223025637Z00'00'\", 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 26.1 (Build 23B85) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20251223025637Z00'00'\", 'title': 'CamScanner 12-22-25 20.52', 'author': 'CamScanner', 'moddate': \"D:20251223025637Z00'00'\", 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 26.1 (Build 23B85) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20251223025637Z00'00'\", 'title': 'CamScanner 12-22-25 20.52', 'author': 'CamScanner', 'moddate': \"D:20251223025637Z00'00'\", 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 26.1 (Build 23B85) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20251223025637Z00'00'\", 'title': 'CamScanner 12-22-25 20.52', 'author': 'CamScanner', 'moddate': \"D:20251223025637Z00'00'\", 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 26.1 (Build 23B85) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20251223025637Z00'00'\", 'title': 'CamScanner 12-22-25 20.52', 'author': 'CamScanner', 'moddate': \"D:20251223025637Z00'00'\", 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 26.1 (Build 23B85) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20251223025637Z00'00'\", 'title': 'CamScanner 12-22-25 20.52', 'author': 'CamScanner', 'moddate': \"D:20251223025637Z00'00'\", 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 26.1 (Build 23B85) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20251223025637Z00'00'\", 'title': 'CamScanner 12-22-25 20.52', 'author': 'CamScanner', 'moddate': \"D:20251223025637Z00'00'\", 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content=''),\n",
       " Document(metadata={'producer': 'iOS Version 26.1 (Build 23B85) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20251223025637Z00'00'\", 'title': 'CamScanner 12-22-25 20.52', 'author': 'CamScanner', 'moddate': \"D:20251223025637Z00'00'\", 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9fd41530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfe5c09",
   "metadata": {},
   "source": [
    "# this is a experimental thing there is no deterministic way to split the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5ca5e333",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "504b296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f844b82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exists: True size: 8940312\n",
      "loader returned count: 11\n",
      "first doc page_content length: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf\"\n",
    "p = Path(file_path)\n",
    "print(\"exists:\", p.exists(), \"size:\", p.stat().st_size if p.exists() else None)\n",
    "\n",
    "loader = PyPDFLoader(str(p))\n",
    "documents = loader.load()\n",
    "print(\"loader returned count:\", len(documents))\n",
    "# show a short sample from first document if present\n",
    "if documents:\n",
    "    print(\"first doc page_content length:\", len(documents[0].page_content))\n",
    "    print(documents[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b4ff379a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 0 extracted chars: 3069\n",
      "page 1 extracted chars: 3084\n",
      "page 1 extracted chars: 3084\n",
      "page 2 extracted chars: 2678\n",
      "page 2 extracted chars: 2678\n",
      "page 3 extracted chars: 2537\n",
      "page 3 extracted chars: 2537\n",
      "page 4 extracted chars: 2247\n",
      "page 4 extracted chars: 2247\n",
      "page 5 extracted chars: 2138\n",
      "page 5 extracted chars: 2138\n",
      "page 6 extracted chars: 1769\n",
      "page 6 extracted chars: 1769\n",
      "page 7 extracted chars: 1252\n",
      "page 7 extracted chars: 1252\n",
      "page 8 extracted chars: 2543\n",
      "page 8 extracted chars: 2543\n",
      "page 9 extracted chars: 3136\n",
      "page 9 extracted chars: 3136\n",
      "page 10 extracted chars: 746\n",
      "OCR documents: 11\n",
      "page 10 extracted chars: 746\n",
      "OCR documents: 11\n"
     ]
    }
   ],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from langchain_core.documents import Document\n",
    "from pathlib import Path\n",
    "\n",
    "p = Path(\"/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf\")\n",
    "pages = convert_from_path(str(p), dpi=200)   # may take time/memory\n",
    "texts = []\n",
    "docs = []\n",
    "for i, img in enumerate(pages):\n",
    "    txt = pytesseract.image_to_string(img)\n",
    "    print(f\"page {i} extracted chars:\", len(txt))\n",
    "    if txt.strip():\n",
    "        docs.append(Document(page_content=txt, metadata={\"page\": i}))\n",
    "# `docs` is a list of langchain Documents you can feed to the splitter\n",
    "print(\"OCR documents:\", len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ee6dafbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'page': 0}, page_content=\"PLAN FOR FUTURE ACTIVITIES IN THE UNITED STATES\\n\\nMy proposed endeavor is to develop, implement, and commercialize advanced\\nretail analytics systems based on computer vision and autonomous store\\ntechnologies in order toenhance operational efficiency, reduce losses, and\\nincrease profitability in the U.S. retail sector. | willleverage my extensive\\nexperience of over a decade in machine learning, MLOps, and multisensory fusion,\\nas well as my doctoral research at Lamar University, “The Role of Computer Vision\\nin Retail Stores”, and my professional work at Babar Inc. LLC and Kaleidoscope\\nInnovation, to further this initiative for the benefit of the United States. My work\\nwill support advancements in artificial intelligence and automation by creating\\ndeployable and scalable computer vision systems that transform how retail\\necosystems and small businesses adopt automation. These systems will be\\nimplemented in industry through pilot programs, edgé-to-cloud deployment\\npipelines (FastAPl + Docker + Kubernetes), and autonomous sensor fusion\\ntechnologies (CCTV + RFID + loT).\\n\\nBy introducing real-time, multi-camera Al systems for customer behavior analysis,\\nproduct recognition, and autonomous payment, | will enable small and medium-\\nsized enterprises to access affordable, sustainable, and innovative automation\\nsolutions, reducing operational costs and improving profitability. Ultimately, my\\nwork willdrive American leadership in artificial intelligence and automation,\\noptimize national technological competitiveness, and contribute to economic\\ngrowth by empowering U.S. retail businesses to thrive in an increasingly\\nautomated global economy.\\n\\nWHY MY PROPOSED ENDEAVOR IS RELEVANT TO THE UNITED STATES\\n\\nThe United States currently faces a series of structural challenges in the retail and\\ntechnology sectors that affect both its economic competitiveness and its leadership\\nin innovation. One of the most significant problems is the sustained increase in\\nretail losses due to theft, inventory shrinkage, and organized crime. According to\\nthe National Retail Federation (NRF), ‘total sector losses reached'$112.1 billion in\\n2022, up from $93.9 billion the previous year, reflecting a growing crisis that\\ndirectly impacts profitability and operational security for businesses nationwide\\n(Retail Dive, 2024). This issue affects not only large chains but also small and\\nmedium-sized enterprises, which lack access to advanced prevention and\\nmonitoring systems, jeopardizing their sustainability and ability to compete in the\\nU.S. market.\\n\\nAdding to this context is the slowdown in physical retail sales, an indicator of strain\\nin the traditional economic consumption model. According to data from the U.S.\\nCensus Bureau (2024),’ total retail sales decreased by 1.6% in September 2024\\ncompared to the same month the previous year, while Trading Economics reported\\n\\n1 https://www.retaildive.com/news/retail-shrink-theft-changed-little-in-2022-nrf/694844/\\n2 https://www.census.gov/retail/mrts/www/statedata/msrs_report_september2024.pdf\\n\\nScanned with\\n: 3 CamScanner’\\n\")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "54127842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR docs: 11\n",
      "first page metadata: {'page': 1, 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'chars': 3069, 'avg_conf': 93.67632850241546}\n",
      "first page preview: PLAN FOR FUTURE ACTIVITIES IN THE UNITED STATES\n",
      "\n",
      "My proposed endeavor is to develop, implement, and commercialize advanced\n",
      "retail analytics systems based on computer vision and autonomous store\n",
      "technologies in order toenhance operational efficiency, reduce losses, and\n",
      "increase profitability in the U.S. retail sector. | willleverage my extensive\n",
      "experience of over a decade in machine learning, MLOps, and multisensory fusion,\n",
      "as well as my doctoral research at Lamar University, “The Role of Computer Vision\n",
      "in Retail Stores”, and my professional work at Babar Inc. LLC and Kaleidoscope\n",
      "Innovation, to further this initiative for the benefit of the United States. My work\n",
      "will support advancements in artificial intelligence and automation by creating\n",
      "deployable and scalable computer vision systems that transform how retail\n",
      "ecosystems and small businesses adopt automation. These systems will be\n",
      "implemented in industry through pilot programs, edgé-to-cloud deployment\n",
      "pipelines (FastAPl + Docker + Kubernetes), and autonomous sensor fusion\n",
      "technologies (CCTV + RFID + loT).\n",
      "\n",
      "By introducing real-time, multi-camera Al systems for customer behavior analysis,\n",
      "product recognition, and autonomous payment, | will enable small and medium-\n",
      "sized enterprises to access affordable, sustainable, and innovative automation\n",
      "solutions, reducing operational costs and improving profitability. Ultimately, my\n",
      "work willdrive American leadership in artificial intelligence and automation,\n",
      "optimize national technological competitiveness, and contribute to economic\n",
      "growth by empowering U.S. retail businesses to thrive in an increasingly\n",
      "automated global economy.\n",
      "\n",
      "WHY MY PROPOSED ENDEAVOR IS RELEVANT TO THE UNITED STATES\n",
      "\n",
      "The United States currently faces a series of structural challenges in the retail and\n",
      "technology sectors that affect both its economic competitiveness and its leadership\n",
      "in innovation. One of the most significant problems is the sustained increase in\n",
      "retail losses due to theft, inventor\n"
     ]
    }
   ],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from langchain_core.documents import Document\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "p = Path(\"/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf\")\n",
    "pages = convert_from_path(str(p), dpi=200)\n",
    "docs = []\n",
    "for i, img in enumerate(pages):\n",
    "    # full OCR text\n",
    "    txt = pytesseract.image_to_string(img)\n",
    "\n",
    "    # per-word OCR info to compute average confidence (if available)\n",
    "    data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)\n",
    "    confs_raw = data.get('conf', [])\n",
    "    confs = []\n",
    "    for c in confs_raw:\n",
    "        try:\n",
    "            # tesseract may return strings or numbers depending on version/setup\n",
    "            val = int(c)\n",
    "            if val >= 0:\n",
    "                confs.append(val)\n",
    "        except Exception:\n",
    "            # skip non-numeric or missing confidence entries\n",
    "            continue\n",
    "    avg_conf = float(np.mean(confs)) if confs else None\n",
    "\n",
    "    if txt.strip():\n",
    "        meta = {\n",
    "            \"page\": i + 1,             # 1-based page number\n",
    "            \"source\": str(p),          # filename / path\n",
    "            \"chars\": len(txt),         # text length\n",
    "            \"avg_conf\": avg_conf       # average OCR confidence (None if not available)\n",
    "        }\n",
    "        docs.append(Document(page_content=txt, metadata=meta))\n",
    "\n",
    "# inspect\n",
    "print(\"OCR docs:\", len(docs))\n",
    "print(\"first page metadata:\", docs[0].metadata)\n",
    "print(\"first page preview:\", docs[0].page_content[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7d517ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"PLAN FOR FUTURE ACTIVITIES IN THE UNITED STATES\\n\\nMy proposed endeavor is to develop, implement, and commercialize advanced\\nretail analytics systems based on computer vision and autonomous store\\ntechnologies in order toenhance operational efficiency, reduce losses, and\\nincrease profitability in the U.S. retail sector. | willleverage my extensive\\nexperience of over a decade in machine learning, MLOps, and multisensory fusion,\\nas well as my doctoral research at Lamar University, “The Role of Computer Vision\\nin Retail Stores”, and my professional work at Babar Inc. LLC and Kaleidoscope\\nInnovation, to further this initiative for the benefit of the United States. My work\\nwill support advancements in artificial intelligence and automation by creating\\ndeployable and scalable computer vision systems that transform how retail\\necosystems and small businesses adopt automation. These systems will be\\nimplemented in industry through pilot programs, edgé-to-cloud deployment\\npipelines (FastAPl + Docker + Kubernetes), and autonomous sensor fusion\\ntechnologies (CCTV + RFID + loT).\\n\\nBy introducing real-time, multi-camera Al systems for customer behavior analysis,\\nproduct recognition, and autonomous payment, | will enable small and medium-\\nsized enterprises to access affordable, sustainable, and innovative automation\\nsolutions, reducing operational costs and improving profitability. Ultimately, my\\nwork willdrive American leadership in artificial intelligence and automation,\\noptimize national technological competitiveness, and contribute to economic\\ngrowth by empowering U.S. retail businesses to thrive in an increasingly\\nautomated global economy.\\n\\nWHY MY PROPOSED ENDEAVOR IS RELEVANT TO THE UNITED STATES\\n\\nThe United States currently faces a series of structural challenges in the retail and\\ntechnology sectors that affect both its economic competitiveness and its leadership\\nin innovation. One of the most significant problems is the sustained increase in\\nretail losses due to theft, inventory shrinkage, and organized crime. According to\\nthe National Retail Federation (NRF), ‘total sector losses reached'$112.1 billion in\\n2022, up from $93.9 billion the previous year, reflecting a growing crisis that\\ndirectly impacts profitability and operational security for businesses nationwide\\n(Retail Dive, 2024). This issue affects not only large chains but also small and\\nmedium-sized enterprises, which lack access to advanced prevention and\\nmonitoring systems, jeopardizing their sustainability and ability to compete in the\\nU.S. market.\\n\\nAdding to this context is the slowdown in physical retail sales, an indicator of strain\\nin the traditional economic consumption model. According to data from the U.S.\\nCensus Bureau (2024),’ total retail sales decreased by 1.6% in September 2024\\ncompared to the same month the previous year, while Trading Economics reported\\n\\n1 https://www.retaildive.com/news/retail-shrink-theft-changed-little-in-2022-nrf/694844/\\n2 https://www.census.gov/retail/mrts/www/statedata/msrs_report_september2024.pdf\\n\\nScanned with\\n: 3 CamScanner’\\n\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "12b4b338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page': 3,\n",
       " 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf',\n",
       " 'chars': 2678,\n",
       " 'avg_conf': 93.14971751412429}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[2].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d45edc11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a monthly contraction of 0.9% in January 2025.° These figures highlight stagnation\\nin physical consumption, forcing U.S. stores to transform through the adoption of\\ntechnologies that optimize operations and enhance customer experience. Without\\neffective technological transition, the loss of competitiveness in physical commerce\\ncould amplify, impacting employment and local economies across multiple regions.\\n\\nSimultaneously, the country is at a decisive moment regarding its global leadership\\nin artificial intelligence and automation. The White House recognized this need in\\nExecutive Order 14179: “Removing Barriers to American Leadership in Artificial\\nIntelligence” (January 2025), ‘stating that maintaining and strengthening American\\ndominance in Al is a strategic matter for economic prosperity and national security,\\nIn this framework, the development of applied technologies—such as computer\\nvision, machine learning, and retail automation—is considered essential to sustain\\nthe United States' competitive advantage against emerging powers. The executive\\norder emphasizes the importance of projects that combine research, development,\\nand commercial implementation, aligning directly with the objectives of this\\nproposal.\\n\\nMoreover, U.S. small and medium-sized enterprises, which represent over 40% of\\nprivate-sector jobs, face difficulties accessing automation solutions due to high\\nimplementation costs and a lack of specialized technical personnel. ‘This\\ntechnological lag creates productivity and competitiveness gaps in local\\necosystems, limiting these businesses’ ability to sustain themselves against large\\ncorporations that adopt Al-based technologies. The lack of accessible innovation\\nfor this segment not only slows economic growth but also weaker's the social and\\nlabor fabric of numerous communities, especially in semi-rural or university\\nregions.\\n\\nMy initiative directly addresses these challenges by developing, implementing, and\\ncommercializing advanced retail analytics systems based on computer vision and\\nautonomous stores in the United States. Specifically, | aim to design and deploy\\nreal-time multi-camera artificial intelligence systems capable of analyzing customer\\nbehavior, recognizing products, and facilitating autonomous payment. These\\nsystems, grounded in my doctoral research at Lamar University and my\\nprofessional experience at Babar Inc. LLC and Kaleidoscope Innovation, will\\nenable loss detection, optimize purchase flows, and reduce operational costs in a\\nscalable manner. The combination of computer vision, machine learning, and\\nmultisensory fusion (CCTV + RFID + loT) will create intelligent retail environments\\nthat help reduce inventory shrinkage and increase operational efficiency,\\naddressing the economic problem of over $112 billion in annual losses. ®\\n\\n3 https://tradingeconomics.com/united-states/retail-sales/news/448020\\n“https://www.whitehouse.gov/presidential-actions/2025/01/ removing-barriers-to-american-leadership-in-\\nartificial-intelligence/\\n\\n5 https://advocacy.sba.gov/category/research/\\n\\nScanned with\\n: 3 CamScanner’\\n\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e1548339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f914d3ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"PLAN FOR FUTURE ACTIVITIES IN THE UNITED STATES\\n\\nMy proposed endeavor is to develop, implement, and commercialize advanced\\nretail analytics systems based on computer vision and autonomous store\\ntechnologies in order toenhance operational efficiency, reduce losses, and\\nincrease profitability in the U.S. retail sector. | willleverage my extensive\\nexperience of over a decade in machine learning, MLOps, and multisensory fusion,\\nas well as my doctoral research at Lamar University, “The Role of Computer Vision\\nin Retail Stores”, and my professional work at Babar Inc. LLC and Kaleidoscope\\nInnovation, to further this initiative for the benefit of the United States. My work\\nwill support advancements in artificial intelligence and automation by creating\\ndeployable and scalable computer vision systems that transform how retail\\necosystems and small businesses adopt automation. These systems will be\\nimplemented in industry through pilot programs, edgé-to-cloud deployment\\npipelines (FastAPl + Docker + Kubernetes), and autonomous sensor fusion\\ntechnologies (CCTV + RFID + loT).\\n\\nBy introducing real-time, multi-camera Al systems for customer behavior analysis,\\nproduct recognition, and autonomous payment, | will enable small and medium-\\nsized enterprises to access affordable, sustainable, and innovative automation\\nsolutions, reducing operational costs and improving profitability. Ultimately, my\\nwork willdrive American leadership in artificial intelligence and automation,\\noptimize national technological competitiveness, and contribute to economic\\ngrowth by empowering U.S. retail businesses to thrive in an increasingly\\nautomated global economy.\\n\\nWHY MY PROPOSED ENDEAVOR IS RELEVANT TO THE UNITED STATES\\n\\nThe United States currently faces a series of structural challenges in the retail and\\ntechnology sectors that affect both its economic competitiveness and its leadership\\nin innovation. One of the most significant problems is the sustained increase in\\nretail losses due to theft, inventory shrinkage, and organized crime. According to\\nthe National Retail Federation (NRF), ‘total sector losses reached'$112.1 billion in\\n2022, up from $93.9 billion the previous year, reflecting a growing crisis that\\ndirectly impacts profitability and operational security for businesses nationwide\\n(Retail Dive, 2024). This issue affects not only large chains but also small and\\nmedium-sized enterprises, which lack access to advanced prevention and\\nmonitoring systems, jeopardizing their sustainability and ability to compete in the\\nU.S. market.\\n\\nAdding to this context is the slowdown in physical retail sales, an indicator of strain\\nin the traditional economic consumption model. According to data from the U.S.\\nCensus Bureau (2024),’ total retail sales decreased by 1.6% in September 2024\\ncompared to the same month the previous year, while Trading Economics reported\\n\\n1 https://www.retaildive.com/news/retail-shrink-theft-changed-little-in-2022-nrf/694844/\\n2 https://www.census.gov/retail/mrts/www/statedata/msrs_report_september2024.pdf\\n\\nScanned with\\n: 3 CamScanner’\\n\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c4ba122a",
   "metadata": {},
   "outputs": [
    {
     "ename": "GoogleGenerativeAIError",
     "evalue": "Error embedding content: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_paid_tier_requests, limit: 3000, model: embedding-001\nPlease retry in 34.516329135s. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_paid_tier_requests\"\n  quota_id: \"EmbedContentPerMinutePerProjectPerUserPerModel-PaidTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"embedding-001\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 3000\n}\n, retry_delay {\n  seconds: 34\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResourceExhausted\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/New Folder/.venv/lib/python3.12/site-packages/langchain_google_genai/embeddings.py:243\u001b[39m, in \u001b[36membed_documents\u001b[39m\u001b[34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[39m\n\u001b[32m    242\u001b[39m headers = {\u001b[33m\"\u001b[39m\u001b[33mUser-Agent\u001b[39m\u001b[33m\"\u001b[39m: user_agent}\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.additional_headers:\n\u001b[32m    244\u001b[39m     headers.update(\u001b[38;5;28mself\u001b[39m.additional_headers)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/New Folder/.venv/lib/python3.12/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:1437\u001b[39m, in \u001b[36mGenerativeServiceClient.batch_embed_contents\u001b[39m\u001b[34m(self, request, model, requests, retry, timeout, metadata)\u001b[39m\n\u001b[32m   1436\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1437\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1438\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1440\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1442\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1444\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/New Folder/.venv/lib/python3.12/site-packages/google/api_core/gapic_v1/method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/New Folder/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/New Folder/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/New Folder/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/New Folder/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/New Folder/.venv/lib/python3.12/site-packages/google/api_core/timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/New Folder/.venv/lib/python3.12/site-packages/google/api_core/grpc_helpers.py:77\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mResourceExhausted\u001b[39m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_paid_tier_requests, limit: 3000, model: embedding-001\nPlease retry in 34.516329135s. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_paid_tier_requests\"\n  quota_id: \"EmbedContentPerMinutePerProjectPerUserPerModel-PaidTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"embedding-001\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 3000\n}\n, retry_delay {\n  seconds: 34\n}\n]",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mGoogleGenerativeAIError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m doc_vector=\u001b[43membedding_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpage_content\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/New Folder/.venv/lib/python3.12/site-packages/langchain_google_genai/embeddings.py:247\u001b[39m, in \u001b[36membed_documents\u001b[39m\u001b[34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.additional_headers:\n\u001b[32m    244\u001b[39m     headers.update(\u001b[38;5;28mself\u001b[39m.additional_headers)\n\u001b[32m    246\u001b[39m http_options = HttpOptions(\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     base_url=\u001b[38;5;28mself\u001b[39m.base_url,\n\u001b[32m    248\u001b[39m     headers=headers,\n\u001b[32m    249\u001b[39m     client_args=\u001b[38;5;28mself\u001b[39m.client_args,\n\u001b[32m    250\u001b[39m     async_client_args=\u001b[38;5;28mself\u001b[39m.client_args,\n\u001b[32m    251\u001b[39m )\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._use_vertexai:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    254\u001b[39m     \u001b[38;5;66;03m# Vertex AI backend\u001b[39;00m\n\u001b[32m    255\u001b[39m     \u001b[38;5;66;03m# Normalize model name - strip 'models/' prefix for Vertex AI\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.startswith(\u001b[33m\"\u001b[39m\u001b[33mmodels/\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mGoogleGenerativeAIError\u001b[39m: Error embedding content: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_paid_tier_requests, limit: 3000, model: embedding-001\nPlease retry in 34.516329135s. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_paid_tier_requests\"\n  quota_id: \"EmbedContentPerMinutePerProjectPerUserPerModel-PaidTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"embedding-001\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 3000\n}\n, retry_delay {\n  seconds: 34\n}\n]"
     ]
    }
   ],
   "source": [
    "doc_vector=embedding_model.embed_documents(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ef5787",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embedding_model.embed_documents(docs[0].page_content)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b29b768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding helper with exponential backoff and parsing retry delay\n",
    "import time, re, random\n",
    "def embed_with_backoff(model, texts, max_retries=6):\n",
    "    \"\"\"Call model.embed_documents(texts) with retries and backoff.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return model.embed_documents(texts)\n",
    "        except Exception as e:\n",
    "            msg = str(e)\n",
    "            # Try to parse explicit retry suggestion like 'Please retry in 34.5s'\n",
    "            m = re.search(r'Please retry in\\s*([0-9.]+)s', msg)\n",
    "            if m:\n",
    "                sleep = float(m.group(1)) + 1.0\n",
    "            else:\n",
    "                # exponential backoff with jitter (seconds)\n",
    "                sleep = min(60, (2 ** attempt) + random.random())\n",
    "            print(f'Embed attempt {attempt+1} failed: {e}; sleeping {sleep:.1f}s')\n",
    "            time.sleep(sleep)\n",
    "    raise RuntimeError('Failed to get embeddings after retries')\n",
    "\n",
    "# Example usage (batch small lists of texts, cache results):\n",
    "# vectors = embed_with_backoff(embedding_model, [docs[0].page_content])\n",
    "# print(len(vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a42a8cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore=FAISS.from_documents(docs, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bcf45ab2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2464702392.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mtoken->words\u001b[39m\n         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "token->words\n",
    "\n",
    "chunk--> it is collection of words(token)[characters]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1063deb2",
   "metadata": {},
   "source": [
    "in memory(faiss is in memory vector store,chroma)\n",
    "on disk storage(faiss you can persist over the disk,chroma)\n",
    "cloud storage(cloud variant of faiss is not available)(pinecone,weaviate,milvus,mongodbvectorsearch,astradb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76ba569",
   "metadata": {},
   "source": [
    "## This is a Retrieval proceesss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "468d9f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_doc=vectorstore.similarity_search(\"llama2 finetuning benchmark experiments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2d49b5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='a2980a91-75a4-4ce1-bad2-04633bf1c315', metadata={'page': 10, 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'chars': 3136, 'avg_conf': 94.5107398568019}, page_content='Academically, | complemented my research work with teaching experience as a\\nGraduate Teaching Assistant at Lamar University, where | taught courses in\\nAutomated System Engineering and Computer-Aided Design and Manufacturing.\\nThis activity allowed me not only to transmit technical knowledge but also to\\ncoordinate interdisciplinary projects between engineering students and local\\nbusinesses, reinforcing my leadership and commitment to technological\\ndissemination.\\n\\nMy profile combines a solid technical foundation with strategic business vision.\\nHaving earned an MBA in Project Management from Northwestern Polytechnic\\nUniversity (California), | possess the management tools necessary to plan, finance,\\nand scale technological initiatives. This combination of engineering, data science,\\nand business management enables me to transform research into commercial\\ninnovation, and innovation into sustainable projects that strengthen the U.S.\\neconomy.\\n\\nAdditionally, | have maintained a consistent scientific output with publications in\\ninternational journals and conferences on computer vision, autonomous robotics,\\nand applied ergonomics. Among them are my articles “MultiModal Product\\nClassification Using YOLO and OCR Fusion” (2024) and “Role of Computer Vision\\nin Retail Stores” (2024), which propose technical frameworks directly applicable to\\nintelligent retail analytics. These contributions have been cited in the context of\\ncommercial automation and confirm my role as a researcher with real impact at the\\ntechnological frontier.\\n\\nThroughout my career, | have received recognitions such as the Research\\nExcellence Fellowship (2021), awarded by Lamar University for academic merit\\nand innovative potential. | also hold professional certifications that strengthen my\\ntechnical profile, including SolidWorks Professional, Software Design Assurance\\n(IEC 62304), and Certified Scrum Master (CSM), which validate my competencies\\nin both complex system design and agile management of technological projects.\\n\\nMy advanced programming expertise in Python, TensorFlow, PyTorch, OpenCV,\\nFAISS, and YOLO architectures enables me to design, train, and deploy intelligent\\nreal-time visual analysis systems. This technical proficiency, combined with my\\nunderstanding of industrial workflows and experience in automated manufacturing,\\npositions me ideally to develop applied technologies that optimize retail operations,\\nreduce losses due to theft or error, and promote a more sustainable and\\ncompetitive commercial ecosystem within the United States.\\n\\nAs part of this proposal, | lead the creation of a fast-food and retail ecosystem\\ndriven by Al, in collaboration with EyeAl and Curry Creations LLC. Our prototype\\ncafé, ready for launch, integrates computer vision, customer behavior analysis, and\\nautonomous payments, demonstrating how intelligent automation can be\\nimplemented at minimal cost through strategic academic-industrial alliances. This\\ninitiative will not only strengthen American technological competitiveness but also\\ngenerate local high-specialization jobs, foster Al adoption in small and medium-\\n\\nScanned with\\n: 3 CamScanner’\\n'),\n",
       " Document(id='64aa8eb8-8f0b-4c7f-b3cd-d796e0726d9a', metadata={'page': 9, 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'chars': 2543, 'avg_conf': 93.0932944606414}, page_content='aa commercial\\net a $80,000 $2,000,000 000,000 Espn ment\\n\\nPROFESSIONAL DEVELOPMENT AND KNOWLEDGE DISSEMINATION\\n\\ne Conferences: AHFE, CVPR, IEEE Computer Vision workshops, and Al\\nsummits for the retail sector.\\n\\n« Research Initiatives: Collaborative projects with Lamar University and\\nindustrial partners in retail automation and sensor fusion.\\n\\n¢ Training Programs: Mentorship for graduate students and workshops on Al\\nretail analytics and MLOps.\\n\\nCONCLUSION\\n\\nWith over a decade of combined experience in academic and industrial settings, |\\npossess a robust and practical foundation to lead the development,\\nimplementation, and commercialization of advanced retail analytics and automation\\nsystems based on artificial intelligence within the United States. My career\\nintegrates applied research, computer vision system design, technological project\\nmanagement, and innovation leadership, uniquely qualifying me to execute this\\nproposal of national impact.\\n\\nMy doctoral training in Industrial and Systems Engineering at Lamar University\\n(2021-2024), where | researched “The Role of Computer Vision in Retail Stores”\\nunder Dr. Xinyu Liu, provided me with a strong technical-scientific framework to\\nunderstand how Al systems can transform operational efficiency in the retail sector.\\nDuring this process, | developed multimodal models combining computer vision\\nand natural language processing, achieving over 93% average accuracy in retail\\nproduct recognition. This work resulted in peer-reviewed publications and the\\ncreation of the Liquor-Grocery dataset, now used to train object detection models\\nin commercial environments. This research positions me not only as a technologist\\nbut as a generator of applied knowledge with immediate market transferability.\\n\\nIn parallel, my professional experience at Vastek Inc. (contracted by Medtronic)\\nenabled me to apply advanced engineering methodologies and software validation\\nstandards (ISO 13485, IEC 62304), implementing vision-controlled automated\\nsystems for medical production lines. This strengthened my expertise in developing\\nregulated, high-reliability products—essential competencies for building safe,\\ntraceable, and scalable retail automation systems. Similarly, at Kaleidoscope\\nInnovation and Babar Inc. LLC, | contributed to the design of commercial solutions\\nintegrating sensors, cameras, RFID, and loT, developing edge-to-cloud\\narchitectures based on FastAPI, Docker, and Kubernetes, focused on the real-\\nworld implementation of Al in retail environments.\\n\\nScanned with\\n: 3 CamScanner’\\n'),\n",
       " Document(id='b8a7ac65-c41a-43b1-9b57-9b72f748453c', metadata={'page': 6, 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'chars': 2138, 'avg_conf': 90.70656370656371}, page_content='The system operates through a structured data flow, including real-time detection\\nprocessing, action analysis, and visualization in an interactive web interface.\\n\\n5, EXPECTED RESULTS AND BENEFITS\\n\\nA. Operational Benefits: Loss prevention, improved inventory accuracy, checkout\\noptimization, and labor efficiency. B. Business Intelligence Benefits: Detailed\\ncustomer behavior analytics, product performance tracking, and planogram\\ncompliance. C. Quantitative Targets: >93% accuracy in object detection, real-time\\nprocessing at 30 FPS, and 15-25% reduction in theft-related losses. D.\\nCompetitive Advantages: Reduced implementation costs, scalability for different\\nstore sizes, and privacy-preserving design.\\n\\n6. COMMERCIALIZATION STRATEGY\\n\\nService Models: SaaS, on-premise deployment, hybrid model, and API\\nlicensing. Market Segmentation: Small and medium-sized retail businesses,\\nregional chains, and large retail chains. Revenue Projections: Progressive scaling\\nfrom pilot implementations to national expansiort. Academic-Industrial\\nCollaboration: Joint research with Lamar University and __ talent\\ndevelopment. Knowledge Dissemination: Technical seminars, open-source\\ncontributions, pilot demonstrations, and training programs.\\n\\nSTRATEGIC EXPANSION\\n\\ne Phase 1 (2025-2026): Implementation in Beaumont, TX.\\n\\ne Phase 2 (2027-2029): Scaling to university-affiliated businesses in Texas\\nand neighboring states.\\n\\ne Phase 3 (2030-2032): National deployment through technology licensing\\nand integration partnerships with major retail chains.\\n\\nTARGET AUDIENCE\\n\\nSegment Purpose / Need Beneficiaries Example\\n\\nReduce losses and inventory\\nRetail companies inaccuracies through Al-based\\nand franchises product recognition and predictive\\n\\nCenvenience stores,\\nsupermarkets, franchise\\n\\nanalytics. chains\\nUniversity- Create living labs for research and Lamar University, UT\\naffiliated training in Al-driven retail Dallas, Texas A&M\\nbusinesses commerce. Retail Ventures\\nMunicipal Demonstrate autonomous service novation hubs in\\ninnovation systems (e.g., delivery robots, smart Baaumont ahd Houston\\ndistricts points of sale).\\n\\nScanned with\\n: 3 CamScanner’\\n'),\n",
       " Document(id='15331d99-1814-45ac-9984-60f0ed61d10a', metadata={'page': 5, 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'chars': 2247, 'avg_conf': 92.49477351916376}, page_content='« Deployment: Docker containerization, Kubernetes orchestration for\\nhorizontal scaling, FastAPI for edge-to-cloud pipelines, Apache Arrow IPC\\nfor zero-copy data serialization.\\n\\n2. SPECIFIC METHODOLOGIES AND TECHNIQUES\\n\\nA. Action Recognition Algorithm The system employs a probabilistic multi-state\\nmachine with temporal consistency and spatial reasoning to detect and classify\\ncustomer actions. Key functions include:\\n\\n¢ Hand-ltem Interaction Detection: Identification of interactions between\\ndetected hands and nearby products, classifying interaction strength as\\nHIGH, MEDIUM, LOW, or NONE based on overlap history (loU).\\n\\n« Probabilistic State Machine: Tracking states such as IDLE, REACHING,\\nGRASPING, PICKED, HOLDING, PLACING, and RETURNED, based on\\noverlap metrics, object velocity, and time-in-state.\\n\\ne Product Identification: Complementary methods of spatial mapping and\\nvisual embedding similarity to assign UPC codes to detected products.\\n\\n« Browsing Detection: Classification of customer behavior as BROWSING\\nbased on stationary position, small gestures, and absence of product\\ninteraction.\\n\\nB. Multi-Camera Sensor Fusion Integration of data from multiple synchronized\\ncameras using temporal alignment and spatial calibration, providing\\ncomprehensive coverage of the retail space and occlusion handling.\\n\\nC. Real-Time Processing Pipeline Optimized for real-time performance using\\nYOLOvé8/v11 models converted to TensorRT format, Parquet metadata storage with\\nefficient indexing, parallel processing with Polars, and video rendering at 60 FPS.\\n\\n3. IMPLEMENTATION PROCESS\\nDevelopment is divided into systematic phases:\\n\\n1. Data Collection and Model Training: Installation of multi-camera systems,\\ncollection of annotated datasets, model training, and optimization.\\n\\n2. System Integration: Deployment of models on edge devices,\\nimplementation of action recognition algorithms, and development of the\\nweb application.\\n\\n3. Pilot Testing: Validation in Babar Inc. locations, parameter tuning, and\\ncollection of performance metrics.\\n\\n4. Scaling and Optimization: Containerization, Kubernetes orchestration,\\ncloud deployment, and API development for third-party integration.\\n\\n4. HOW THE SYSTEM CURRENTLY OPERATES\\n\\nScanned with\\n| @ CamScanner’\\n')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e573fed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Academically, | complemented my research work with teaching experience as a\\nGraduate Teaching Assistant at Lamar University, where | taught courses in\\nAutomated System Engineering and Computer-Aided Design and Manufacturing.\\nThis activity allowed me not only to transmit technical knowledge but also to\\ncoordinate interdisciplinary projects between engineering students and local\\nbusinesses, reinforcing my leadership and commitment to technological\\ndissemination.\\n\\nMy profile combines a solid technical foundation with strategic business vision.\\nHaving earned an MBA in Project Management from Northwestern Polytechnic\\nUniversity (California), | possess the management tools necessary to plan, finance,\\nand scale technological initiatives. This combination of engineering, data science,\\nand business management enables me to transform research into commercial\\ninnovation, and innovation into sustainable projects that strengthen the U.S.\\neconomy.\\n\\nAdditionally, | have maintained a consistent scientific output with publications in\\ninternational journals and conferences on computer vision, autonomous robotics,\\nand applied ergonomics. Among them are my articles “MultiModal Product\\nClassification Using YOLO and OCR Fusion” (2024) and “Role of Computer Vision\\nin Retail Stores” (2024), which propose technical frameworks directly applicable to\\nintelligent retail analytics. These contributions have been cited in the context of\\ncommercial automation and confirm my role as a researcher with real impact at the\\ntechnological frontier.\\n\\nThroughout my career, | have received recognitions such as the Research\\nExcellence Fellowship (2021), awarded by Lamar University for academic merit\\nand innovative potential. | also hold professional certifications that strengthen my\\ntechnical profile, including SolidWorks Professional, Software Design Assurance\\n(IEC 62304), and Certified Scrum Master (CSM), which validate my competencies\\nin both complex system design and agile management of technological projects.\\n\\nMy advanced programming expertise in Python, TensorFlow, PyTorch, OpenCV,\\nFAISS, and YOLO architectures enables me to design, train, and deploy intelligent\\nreal-time visual analysis systems. This technical proficiency, combined with my\\nunderstanding of industrial workflows and experience in automated manufacturing,\\npositions me ideally to develop applied technologies that optimize retail operations,\\nreduce losses due to theft or error, and promote a more sustainable and\\ncompetitive commercial ecosystem within the United States.\\n\\nAs part of this proposal, | lead the creation of a fast-food and retail ecosystem\\ndriven by Al, in collaboration with EyeAl and Curry Creations LLC. Our prototype\\ncafé, ready for launch, integrates computer vision, customer behavior analysis, and\\nautonomous payments, demonstrating how intelligent automation can be\\nimplemented at minimal cost through strategic academic-industrial alliances. This\\ninitiative will not only strengthen American technological competitiveness but also\\ngenerate local high-specialization jobs, foster Al adoption in small and medium-\\n\\nScanned with\\n: 3 CamScanner’\\n'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "752da306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aa commercial\\net a $80,000 $2,000,000 000,000 Espn ment\\n\\nPROFESSIONAL DEVELOPMENT AND KNOWLEDGE DISSEMINATION\\n\\ne Conferences: AHFE, CVPR, IEEE Computer Vision workshops, and Al\\nsummits for the retail sector.\\n\\n« Research Initiatives: Collaborative projects with Lamar University and\\nindustrial partners in retail automation and sensor fusion.\\n\\n¢ Training Programs: Mentorship for graduate students and workshops on Al\\nretail analytics and MLOps.\\n\\nCONCLUSION\\n\\nWith over a decade of combined experience in academic and industrial settings, |\\npossess a robust and practical foundation to lead the development,\\nimplementation, and commercialization of advanced retail analytics and automation\\nsystems based on artificial intelligence within the United States. My career\\nintegrates applied research, computer vision system design, technological project\\nmanagement, and innovation leadership, uniquely qualifying me to execute this\\nproposal of national impact.\\n\\nMy doctoral training in Industrial and Systems Engineering at Lamar University\\n(2021-2024), where | researched “The Role of Computer Vision in Retail Stores”\\nunder Dr. Xinyu Liu, provided me with a strong technical-scientific framework to\\nunderstand how Al systems can transform operational efficiency in the retail sector.\\nDuring this process, | developed multimodal models combining computer vision\\nand natural language processing, achieving over 93% average accuracy in retail\\nproduct recognition. This work resulted in peer-reviewed publications and the\\ncreation of the Liquor-Grocery dataset, now used to train object detection models\\nin commercial environments. This research positions me not only as a technologist\\nbut as a generator of applied knowledge with immediate market transferability.\\n\\nIn parallel, my professional experience at Vastek Inc. (contracted by Medtronic)\\nenabled me to apply advanced engineering methodologies and software validation\\nstandards (ISO 13485, IEC 62304), implementing vision-controlled automated\\nsystems for medical production lines. This strengthened my expertise in developing\\nregulated, high-reliability products—essential competencies for building safe,\\ntraceable, and scalable retail automation systems. Similarly, at Kaleidoscope\\nInnovation and Babar Inc. LLC, | contributed to the design of commercial solutions\\nintegrating sensors, cameras, RFID, and loT, developing edge-to-cloud\\narchitectures based on FastAPI, Docker, and Kubernetes, focused on the real-\\nworld implementation of Al in retail environments.\\n\\nScanned with\\n: 3 CamScanner’\\n'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "967c797a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The system operates through a structured data flow, including real-time detection\\nprocessing, action analysis, and visualization in an interactive web interface.\\n\\n5, EXPECTED RESULTS AND BENEFITS\\n\\nA. Operational Benefits: Loss prevention, improved inventory accuracy, checkout\\noptimization, and labor efficiency. B. Business Intelligence Benefits: Detailed\\ncustomer behavior analytics, product performance tracking, and planogram\\ncompliance. C. Quantitative Targets: >93% accuracy in object detection, real-time\\nprocessing at 30 FPS, and 15-25% reduction in theft-related losses. D.\\nCompetitive Advantages: Reduced implementation costs, scalability for different\\nstore sizes, and privacy-preserving design.\\n\\n6. COMMERCIALIZATION STRATEGY\\n\\nService Models: SaaS, on-premise deployment, hybrid model, and API\\nlicensing. Market Segmentation: Small and medium-sized retail businesses,\\nregional chains, and large retail chains. Revenue Projections: Progressive scaling\\nfrom pilot implementations to national expansiort. Academic-Industrial\\nCollaboration: Joint research with Lamar University and __ talent\\ndevelopment. Knowledge Dissemination: Technical seminars, open-source\\ncontributions, pilot demonstrations, and training programs.\\n\\nSTRATEGIC EXPANSION\\n\\ne Phase 1 (2025-2026): Implementation in Beaumont, TX.\\n\\ne Phase 2 (2027-2029): Scaling to university-affiliated businesses in Texas\\nand neighboring states.\\n\\ne Phase 3 (2030-2032): National deployment through technology licensing\\nand integration partnerships with major retail chains.\\n\\nTARGET AUDIENCE\\n\\nSegment Purpose / Need Beneficiaries Example\\n\\nReduce losses and inventory\\nRetail companies inaccuracies through Al-based\\nand franchises product recognition and predictive\\n\\nCenvenience stores,\\nsupermarkets, franchise\\n\\nanalytics. chains\\nUniversity- Create living labs for research and Lamar University, UT\\naffiliated training in Al-driven retail Dallas, Texas A&M\\nbusinesses commerce. Retail Ventures\\nMunicipal Demonstrate autonomous service novation hubs in\\ninnovation systems (e.g., delivery robots, smart Baaumont ahd Houston\\ndistricts points of sale).\\n\\nScanned with\\n: 3 CamScanner’\\n'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc[2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6a241033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'« Deployment: Docker containerization, Kubernetes orchestration for\\nhorizontal scaling, FastAPI for edge-to-cloud pipelines, Apache Arrow IPC\\nfor zero-copy data serialization.\\n\\n2. SPECIFIC METHODOLOGIES AND TECHNIQUES\\n\\nA. Action Recognition Algorithm The system employs a probabilistic multi-state\\nmachine with temporal consistency and spatial reasoning to detect and classify\\ncustomer actions. Key functions include:\\n\\n¢ Hand-ltem Interaction Detection: Identification of interactions between\\ndetected hands and nearby products, classifying interaction strength as\\nHIGH, MEDIUM, LOW, or NONE based on overlap history (loU).\\n\\n« Probabilistic State Machine: Tracking states such as IDLE, REACHING,\\nGRASPING, PICKED, HOLDING, PLACING, and RETURNED, based on\\noverlap metrics, object velocity, and time-in-state.\\n\\ne Product Identification: Complementary methods of spatial mapping and\\nvisual embedding similarity to assign UPC codes to detected products.\\n\\n« Browsing Detection: Classification of customer behavior as BROWSING\\nbased on stationary position, small gestures, and absence of product\\ninteraction.\\n\\nB. Multi-Camera Sensor Fusion Integration of data from multiple synchronized\\ncameras using temporal alignment and spatial calibration, providing\\ncomprehensive coverage of the retail space and occlusion handling.\\n\\nC. Real-Time Processing Pipeline Optimized for real-time performance using\\nYOLOvé8/v11 models converted to TensorRT format, Parquet metadata storage with\\nefficient indexing, parallel processing with Polars, and video rendering at 60 FPS.\\n\\n3. IMPLEMENTATION PROCESS\\nDevelopment is divided into systematic phases:\\n\\n1. Data Collection and Model Training: Installation of multi-camera systems,\\ncollection of annotated datasets, model training, and optimization.\\n\\n2. System Integration: Deployment of models on edge devices,\\nimplementation of action recognition algorithms, and development of the\\nweb application.\\n\\n3. Pilot Testing: Validation in Babar Inc. locations, parameter tuning, and\\ncollection of performance metrics.\\n\\n4. Scaling and Optimization: Containerization, Kubernetes orchestration,\\ncloud deployment, and API development for third-party integration.\\n\\n4. HOW THE SYSTEM CURRENTLY OPERATES\\n\\nScanned with\\n| @ CamScanner’\\n'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc[3].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "871ffe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_doc=vectorstore.similarity_search(\"llama2 finetuning benchmark experiments.\",k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e81d1282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='a2980a91-75a4-4ce1-bad2-04633bf1c315', metadata={'page': 10, 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'chars': 3136, 'avg_conf': 94.5107398568019}, page_content='Academically, | complemented my research work with teaching experience as a\\nGraduate Teaching Assistant at Lamar University, where | taught courses in\\nAutomated System Engineering and Computer-Aided Design and Manufacturing.\\nThis activity allowed me not only to transmit technical knowledge but also to\\ncoordinate interdisciplinary projects between engineering students and local\\nbusinesses, reinforcing my leadership and commitment to technological\\ndissemination.\\n\\nMy profile combines a solid technical foundation with strategic business vision.\\nHaving earned an MBA in Project Management from Northwestern Polytechnic\\nUniversity (California), | possess the management tools necessary to plan, finance,\\nand scale technological initiatives. This combination of engineering, data science,\\nand business management enables me to transform research into commercial\\ninnovation, and innovation into sustainable projects that strengthen the U.S.\\neconomy.\\n\\nAdditionally, | have maintained a consistent scientific output with publications in\\ninternational journals and conferences on computer vision, autonomous robotics,\\nand applied ergonomics. Among them are my articles “MultiModal Product\\nClassification Using YOLO and OCR Fusion” (2024) and “Role of Computer Vision\\nin Retail Stores” (2024), which propose technical frameworks directly applicable to\\nintelligent retail analytics. These contributions have been cited in the context of\\ncommercial automation and confirm my role as a researcher with real impact at the\\ntechnological frontier.\\n\\nThroughout my career, | have received recognitions such as the Research\\nExcellence Fellowship (2021), awarded by Lamar University for academic merit\\nand innovative potential. | also hold professional certifications that strengthen my\\ntechnical profile, including SolidWorks Professional, Software Design Assurance\\n(IEC 62304), and Certified Scrum Master (CSM), which validate my competencies\\nin both complex system design and agile management of technological projects.\\n\\nMy advanced programming expertise in Python, TensorFlow, PyTorch, OpenCV,\\nFAISS, and YOLO architectures enables me to design, train, and deploy intelligent\\nreal-time visual analysis systems. This technical proficiency, combined with my\\nunderstanding of industrial workflows and experience in automated manufacturing,\\npositions me ideally to develop applied technologies that optimize retail operations,\\nreduce losses due to theft or error, and promote a more sustainable and\\ncompetitive commercial ecosystem within the United States.\\n\\nAs part of this proposal, | lead the creation of a fast-food and retail ecosystem\\ndriven by Al, in collaboration with EyeAl and Curry Creations LLC. Our prototype\\ncafé, ready for launch, integrates computer vision, customer behavior analysis, and\\nautonomous payments, demonstrating how intelligent automation can be\\nimplemented at minimal cost through strategic academic-industrial alliances. This\\ninitiative will not only strengthen American technological competitiveness but also\\ngenerate local high-specialization jobs, foster Al adoption in small and medium-\\n\\nScanned with\\n: 3 CamScanner’\\n'),\n",
       " Document(id='64aa8eb8-8f0b-4c7f-b3cd-d796e0726d9a', metadata={'page': 9, 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'chars': 2543, 'avg_conf': 93.0932944606414}, page_content='aa commercial\\net a $80,000 $2,000,000 000,000 Espn ment\\n\\nPROFESSIONAL DEVELOPMENT AND KNOWLEDGE DISSEMINATION\\n\\ne Conferences: AHFE, CVPR, IEEE Computer Vision workshops, and Al\\nsummits for the retail sector.\\n\\n« Research Initiatives: Collaborative projects with Lamar University and\\nindustrial partners in retail automation and sensor fusion.\\n\\n¢ Training Programs: Mentorship for graduate students and workshops on Al\\nretail analytics and MLOps.\\n\\nCONCLUSION\\n\\nWith over a decade of combined experience in academic and industrial settings, |\\npossess a robust and practical foundation to lead the development,\\nimplementation, and commercialization of advanced retail analytics and automation\\nsystems based on artificial intelligence within the United States. My career\\nintegrates applied research, computer vision system design, technological project\\nmanagement, and innovation leadership, uniquely qualifying me to execute this\\nproposal of national impact.\\n\\nMy doctoral training in Industrial and Systems Engineering at Lamar University\\n(2021-2024), where | researched “The Role of Computer Vision in Retail Stores”\\nunder Dr. Xinyu Liu, provided me with a strong technical-scientific framework to\\nunderstand how Al systems can transform operational efficiency in the retail sector.\\nDuring this process, | developed multimodal models combining computer vision\\nand natural language processing, achieving over 93% average accuracy in retail\\nproduct recognition. This work resulted in peer-reviewed publications and the\\ncreation of the Liquor-Grocery dataset, now used to train object detection models\\nin commercial environments. This research positions me not only as a technologist\\nbut as a generator of applied knowledge with immediate market transferability.\\n\\nIn parallel, my professional experience at Vastek Inc. (contracted by Medtronic)\\nenabled me to apply advanced engineering methodologies and software validation\\nstandards (ISO 13485, IEC 62304), implementing vision-controlled automated\\nsystems for medical production lines. This strengthened my expertise in developing\\nregulated, high-reliability products—essential competencies for building safe,\\ntraceable, and scalable retail automation systems. Similarly, at Kaleidoscope\\nInnovation and Babar Inc. LLC, | contributed to the design of commercial solutions\\nintegrating sensors, cameras, RFID, and loT, developing edge-to-cloud\\narchitectures based on FastAPI, Docker, and Kubernetes, focused on the real-\\nworld implementation of Al in retail environments.\\n\\nScanned with\\n: 3 CamScanner’\\n'),\n",
       " Document(id='b8a7ac65-c41a-43b1-9b57-9b72f748453c', metadata={'page': 6, 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'chars': 2138, 'avg_conf': 90.70656370656371}, page_content='The system operates through a structured data flow, including real-time detection\\nprocessing, action analysis, and visualization in an interactive web interface.\\n\\n5, EXPECTED RESULTS AND BENEFITS\\n\\nA. Operational Benefits: Loss prevention, improved inventory accuracy, checkout\\noptimization, and labor efficiency. B. Business Intelligence Benefits: Detailed\\ncustomer behavior analytics, product performance tracking, and planogram\\ncompliance. C. Quantitative Targets: >93% accuracy in object detection, real-time\\nprocessing at 30 FPS, and 15-25% reduction in theft-related losses. D.\\nCompetitive Advantages: Reduced implementation costs, scalability for different\\nstore sizes, and privacy-preserving design.\\n\\n6. COMMERCIALIZATION STRATEGY\\n\\nService Models: SaaS, on-premise deployment, hybrid model, and API\\nlicensing. Market Segmentation: Small and medium-sized retail businesses,\\nregional chains, and large retail chains. Revenue Projections: Progressive scaling\\nfrom pilot implementations to national expansiort. Academic-Industrial\\nCollaboration: Joint research with Lamar University and __ talent\\ndevelopment. Knowledge Dissemination: Technical seminars, open-source\\ncontributions, pilot demonstrations, and training programs.\\n\\nSTRATEGIC EXPANSION\\n\\ne Phase 1 (2025-2026): Implementation in Beaumont, TX.\\n\\ne Phase 2 (2027-2029): Scaling to university-affiliated businesses in Texas\\nand neighboring states.\\n\\ne Phase 3 (2030-2032): National deployment through technology licensing\\nand integration partnerships with major retail chains.\\n\\nTARGET AUDIENCE\\n\\nSegment Purpose / Need Beneficiaries Example\\n\\nReduce losses and inventory\\nRetail companies inaccuracies through Al-based\\nand franchises product recognition and predictive\\n\\nCenvenience stores,\\nsupermarkets, franchise\\n\\nanalytics. chains\\nUniversity- Create living labs for research and Lamar University, UT\\naffiliated training in Al-driven retail Dallas, Texas A&M\\nbusinesses commerce. Retail Ventures\\nMunicipal Demonstrate autonomous service novation hubs in\\ninnovation systems (e.g., delivery robots, smart Baaumont ahd Houston\\ndistricts points of sale).\\n\\nScanned with\\n: 3 CamScanner’\\n'),\n",
       " Document(id='15331d99-1814-45ac-9984-60f0ed61d10a', metadata={'page': 5, 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'chars': 2247, 'avg_conf': 92.49477351916376}, page_content='« Deployment: Docker containerization, Kubernetes orchestration for\\nhorizontal scaling, FastAPI for edge-to-cloud pipelines, Apache Arrow IPC\\nfor zero-copy data serialization.\\n\\n2. SPECIFIC METHODOLOGIES AND TECHNIQUES\\n\\nA. Action Recognition Algorithm The system employs a probabilistic multi-state\\nmachine with temporal consistency and spatial reasoning to detect and classify\\ncustomer actions. Key functions include:\\n\\n¢ Hand-ltem Interaction Detection: Identification of interactions between\\ndetected hands and nearby products, classifying interaction strength as\\nHIGH, MEDIUM, LOW, or NONE based on overlap history (loU).\\n\\n« Probabilistic State Machine: Tracking states such as IDLE, REACHING,\\nGRASPING, PICKED, HOLDING, PLACING, and RETURNED, based on\\noverlap metrics, object velocity, and time-in-state.\\n\\ne Product Identification: Complementary methods of spatial mapping and\\nvisual embedding similarity to assign UPC codes to detected products.\\n\\n« Browsing Detection: Classification of customer behavior as BROWSING\\nbased on stationary position, small gestures, and absence of product\\ninteraction.\\n\\nB. Multi-Camera Sensor Fusion Integration of data from multiple synchronized\\ncameras using temporal alignment and spatial calibration, providing\\ncomprehensive coverage of the retail space and occlusion handling.\\n\\nC. Real-Time Processing Pipeline Optimized for real-time performance using\\nYOLOvé8/v11 models converted to TensorRT format, Parquet metadata storage with\\nefficient indexing, parallel processing with Polars, and video rendering at 60 FPS.\\n\\n3. IMPLEMENTATION PROCESS\\nDevelopment is divided into systematic phases:\\n\\n1. Data Collection and Model Training: Installation of multi-camera systems,\\ncollection of annotated datasets, model training, and optimization.\\n\\n2. System Integration: Deployment of models on edge devices,\\nimplementation of action recognition algorithms, and development of the\\nweb application.\\n\\n3. Pilot Testing: Validation in Babar Inc. locations, parameter tuning, and\\ncollection of performance metrics.\\n\\n4. Scaling and Optimization: Containerization, Kubernetes orchestration,\\ncloud deployment, and API development for third-party integration.\\n\\n4. HOW THE SYSTEM CURRENTLY OPERATES\\n\\nScanned with\\n| @ CamScanner’\\n'),\n",
       " Document(id='e5ac10f3-f685-4e94-8c0a-00a8e19cc85f', metadata={'page': 8, 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'chars': 1252, 'avg_conf': 83.33502538071066}, page_content='ORGANIZATIONAL STRUCTURE\\n\\nfe Annual Weekly ine Year of\\nn Key R i\\nPositio Salary isis y Responsibilities Incorporation\\nSaumil Patel - R&D leadership, client\\nFounder & acquisition, model\\nPrincipal $40,000 30 architecture, and project Year 1\\nEngineer supervision.\\n$40,000-$ Model training and\\nInterns (2) / : :\\nKitchen Operator 50,000 20-40 implementation, data Year 2\\neach engineering.\\nBusiness Client relations,\\nDevelopment $50,000 40 marketing, and financing Year 3\\nManager (Intern) coordination.\\nPROJECTED INITIAL COSTS\\nDescription fen)\\nCommercial space and lab setup $30,000\\nPerimeter Al hardware (GPUs, Jetsons, $20,000\\ncameras)\\nSoftware licenses / cloud infrastructure $10,000\\nMarketing and networking events $500\\nOperational expenses and permits $9,500\\nTotal $70,000\\n\\nFunding Sources: Personal savings and private investment; equipment and\\ncommercial space provided through partnership with Babar Inc. LLC.\\n\\nFINANCIAL PROJECTIONS (5-Year Horizon)\\n: Average Total\\nYear men! Pe cee. lIRevenue per|Revenue Notes\\nP Client (USD)\\n$0 Initial phase: R&D +\\nlaunch\\nExpansion with local\\n$65,000 $130,000 bariners\\n\\n—\\n2\\n(202899 2s enn is Fal\\n3 Regional\\n(2027) Bs ince __{5s00,000 implementations\\n9 $75,000 $750,000 Scaling in Texas\\n\\nScanned with\\n© CamScanner™\\n'),\n",
       " Document(id='1a300c49-04be-42cb-8b21-6aadb36c75ab', metadata={'page': 4, 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'chars': 2537, 'avg_conf': 92.88571428571429}, page_content='The proposed retail analytics system, ActionLens Studio, is a full-stack video\\nanalysis platform built on a modern, scalable architecture that combines computer\\nvision, machine learning, and cloud computing technologies. The system\\narchitecture consists of five integrated layers:\\n\\nPresentation Layer (Frontend): Built with vanilla JavaScript (ES6+), HTML5\\nVideo API, and Canvas 2D API for real-time video playback and detection\\nvisualization. This layer provides an interactive web interface where users can\\nupload videos, view detections overlaid on video frames, navigate through tracked\\nobjects, and analyze customer actions.\\n\\nAPI Layer: Implemented using FastHTML (Python web framework) with Uvicorn\\nASGI server, providing RESTful endpoints for video upload, frame metadata\\nmanagement, action recognition triggers, and data retrieval.\\n\\nBusiness Logic Layer: Contains the core action recognition algorithms including\\nActionRecognitionService, DataframeManager (Polars), DetectionLoader, and\\nFrameMetaLoader. This layer implements the state machine logic, probabilistic\\nmodeling, and temporal reasoning that drives customer action detection.\\n\\nDomain Layer (Action Recognition): Comprises specialized modules including\\nActionRecManager, ActionRecProcessor, HandinteractionManager,\\nItemInteractionManager, BrowsingDetector, and ProbabilisticActionState. These\\ncomponents. implement sophisticated algorithms for detecting and classifying\\ncustomer actions.\\n\\nData Access Layer: Utilizes Polars for high-performance dataframe operations\\n(10-100x faster than Pandas), Apache Parquet for columnar storage, Milvus vector\\ndatabase for product embedding similarity search, and Azure Blob Storage for\\ncloud data persistence.\\n\\nCore Technology Stack:\\n\\ne Backend: Python 3.9+, FastHTML 0.4.0+, Uvicorn ASGI server, Polars\\n0.20.0+ (dataframes), PyArrow 14.0+ (columnar data), Shapely 2.0+\\n(geometric operations), NumPy/SciPy (numerical operations), Pymilvus 2.5+\\n(vector database client), Azure SDK 12.19+ (cloud storage).\\n\\n« Computer Vision Models: YOLOv8 and YOLOv11 optimized with\\nTensorRT for real-time object detection, achieving >93% accuracy in retail\\nproduct recognition. The models are deployed using edge computing\\narchitectures for low-latency inference.\\n\\ne Data Storage: Parquet (columnar format for frame ‘metadata with\\npartitioning by frame_id in chunks of 1000 for O(1) lookups), Milvus (vector\\ndatabase for product embeddings with L2 distance metric), Azure Blob\\nStorage (scalable cloud object storage).\\n\\nScanned with\\n: 3 CamScanner’\\n'),\n",
       " Document(id='0ab82607-8401-48b4-9bf6-d80dcb354572', metadata={'page': 1, 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'chars': 3069, 'avg_conf': 93.67632850241546}, page_content=\"PLAN FOR FUTURE ACTIVITIES IN THE UNITED STATES\\n\\nMy proposed endeavor is to develop, implement, and commercialize advanced\\nretail analytics systems based on computer vision and autonomous store\\ntechnologies in order toenhance operational efficiency, reduce losses, and\\nincrease profitability in the U.S. retail sector. | willleverage my extensive\\nexperience of over a decade in machine learning, MLOps, and multisensory fusion,\\nas well as my doctoral research at Lamar University, “The Role of Computer Vision\\nin Retail Stores”, and my professional work at Babar Inc. LLC and Kaleidoscope\\nInnovation, to further this initiative for the benefit of the United States. My work\\nwill support advancements in artificial intelligence and automation by creating\\ndeployable and scalable computer vision systems that transform how retail\\necosystems and small businesses adopt automation. These systems will be\\nimplemented in industry through pilot programs, edgé-to-cloud deployment\\npipelines (FastAPl + Docker + Kubernetes), and autonomous sensor fusion\\ntechnologies (CCTV + RFID + loT).\\n\\nBy introducing real-time, multi-camera Al systems for customer behavior analysis,\\nproduct recognition, and autonomous payment, | will enable small and medium-\\nsized enterprises to access affordable, sustainable, and innovative automation\\nsolutions, reducing operational costs and improving profitability. Ultimately, my\\nwork willdrive American leadership in artificial intelligence and automation,\\noptimize national technological competitiveness, and contribute to economic\\ngrowth by empowering U.S. retail businesses to thrive in an increasingly\\nautomated global economy.\\n\\nWHY MY PROPOSED ENDEAVOR IS RELEVANT TO THE UNITED STATES\\n\\nThe United States currently faces a series of structural challenges in the retail and\\ntechnology sectors that affect both its economic competitiveness and its leadership\\nin innovation. One of the most significant problems is the sustained increase in\\nretail losses due to theft, inventory shrinkage, and organized crime. According to\\nthe National Retail Federation (NRF), ‘total sector losses reached'$112.1 billion in\\n2022, up from $93.9 billion the previous year, reflecting a growing crisis that\\ndirectly impacts profitability and operational security for businesses nationwide\\n(Retail Dive, 2024). This issue affects not only large chains but also small and\\nmedium-sized enterprises, which lack access to advanced prevention and\\nmonitoring systems, jeopardizing their sustainability and ability to compete in the\\nU.S. market.\\n\\nAdding to this context is the slowdown in physical retail sales, an indicator of strain\\nin the traditional economic consumption model. According to data from the U.S.\\nCensus Bureau (2024),’ total retail sales decreased by 1.6% in September 2024\\ncompared to the same month the previous year, while Trading Economics reported\\n\\n1 https://www.retaildive.com/news/retail-shrink-theft-changed-little-in-2022-nrf/694844/\\n2 https://www.census.gov/retail/mrts/www/statedata/msrs_report_september2024.pdf\\n\\nScanned with\\n: 3 CamScanner’\\n\"),\n",
       " Document(id='83338117-135c-49a9-8dc2-1a2e0a0621a2', metadata={'page': 2, 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'chars': 3084, 'avg_conf': 93.80939947780679}, page_content=\"a monthly contraction of 0.9% in January 2025.° These figures highlight stagnation\\nin physical consumption, forcing U.S. stores to transform through the adoption of\\ntechnologies that optimize operations and enhance customer experience. Without\\neffective technological transition, the loss of competitiveness in physical commerce\\ncould amplify, impacting employment and local economies across multiple regions.\\n\\nSimultaneously, the country is at a decisive moment regarding its global leadership\\nin artificial intelligence and automation. The White House recognized this need in\\nExecutive Order 14179: “Removing Barriers to American Leadership in Artificial\\nIntelligence” (January 2025), ‘stating that maintaining and strengthening American\\ndominance in Al is a strategic matter for economic prosperity and national security,\\nIn this framework, the development of applied technologies—such as computer\\nvision, machine learning, and retail automation—is considered essential to sustain\\nthe United States' competitive advantage against emerging powers. The executive\\norder emphasizes the importance of projects that combine research, development,\\nand commercial implementation, aligning directly with the objectives of this\\nproposal.\\n\\nMoreover, U.S. small and medium-sized enterprises, which represent over 40% of\\nprivate-sector jobs, face difficulties accessing automation solutions due to high\\nimplementation costs and a lack of specialized technical personnel. ‘This\\ntechnological lag creates productivity and competitiveness gaps in local\\necosystems, limiting these businesses’ ability to sustain themselves against large\\ncorporations that adopt Al-based technologies. The lack of accessible innovation\\nfor this segment not only slows economic growth but also weaker's the social and\\nlabor fabric of numerous communities, especially in semi-rural or university\\nregions.\\n\\nMy initiative directly addresses these challenges by developing, implementing, and\\ncommercializing advanced retail analytics systems based on computer vision and\\nautonomous stores in the United States. Specifically, | aim to design and deploy\\nreal-time multi-camera artificial intelligence systems capable of analyzing customer\\nbehavior, recognizing products, and facilitating autonomous payment. These\\nsystems, grounded in my doctoral research at Lamar University and my\\nprofessional experience at Babar Inc. LLC and Kaleidoscope Innovation, will\\nenable loss detection, optimize purchase flows, and reduce operational costs in a\\nscalable manner. The combination of computer vision, machine learning, and\\nmultisensory fusion (CCTV + RFID + loT) will create intelligent retail environments\\nthat help reduce inventory shrinkage and increase operational efficiency,\\naddressing the economic problem of over $112 billion in annual losses. ®\\n\\n3 https://tradingeconomics.com/united-states/retail-sales/news/448020\\n“https://www.whitehouse.gov/presidential-actions/2025/01/ removing-barriers-to-american-leadership-in-\\nartificial-intelligence/\\n\\n5 https://advocacy.sba.gov/category/research/\\n\\nScanned with\\n: 3 CamScanner’\\n\"),\n",
       " Document(id='c5d9a3a9-162e-480f-8637-e2869655dcc6', metadata={'page': 3, 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'chars': 2678, 'avg_conf': 93.14971751412429}, page_content='The project also seeks to revitalize physical retail sales by offering analytical tools\\nthat optimize customer experience through behavior pattern detection, improved\\nspace layout, and payment process automation. This enhances the\\ncompetitiveness of physical stores against the rise of e-commerce, driving a more\\nefficient and sustainable hybrid model. By implementing edge-to-cloud deployment\\npipelines (FastAPI, Docker, Kubernetes) and optimized Al models (YOLOv8/v11 >\\nTensorRT), the proposal aligns with federal priorities to boost American leadership\\nin artificial intelligence, contributing to both technological innovation and the\\ncreation of advanced automation infrastructure.\\n\\nFinally, by starting with a pilot project in Beaumont, Texas, near Lamar University,\\nthis initiative will foster academic-industrial collaboration and offer accessible\\nsolutions to small and medium-sized enterprises, derhocratizing access to\\nintelligent automation. This approach drives local economic development, creates\\ntechnological jobs, and strengthens the regional innovation ecosystem, directly\\ncontributing to the national goals of sustainability, competitiveness, and leadership\\nin artificial intelligence established by the U.S. government.\\n\\nSTEPS TO IMPLEMENT MY ENDEAVOR\\n\\n| will operate as an engineer and founder of CurryCreations, a technology-focused\\ncompany specializing in advanced retail analytics systems, computer vision,\\nartificial intelligence, and autonomous store management solutions. The company\\noperates under the brand name EyeAl Solutions for its technology and _analytics\\nservices. EyeAl Solutions LLC is headquartered in Beaumont, Texas, a strategic\\nlocation near Lamar University that provides access to technical .talent and a real-\\nworld testing ecosystem (the Babar Inc. retail cluster and Kampus Korner\\ncomplex).\\n\\nThe first testing site, a fully equipped container café and adjacent retail store, is\\nready in Beaumont, Texas, awaiting final permits within the next 15 days. This\\nspace is backed by Babar Inc., a food and retail company with over 30 years of\\nexperience in Beaumont, which provides commercial space, equipment, and\\noperational mentorship as part of a strategic alliance, reducing initial investment\\nand maximizing local expertise.\\n\\nThe venture will develop Al-driven retail automation systems through a hybrid\\nmodel of R&D collaboration and commercial implementation, focusing on edge\\ncomputing solutions and sensor fusion for autonomous retail and dining operations.\\nCOMPREHENSIVE TECHNICAL DETAIL\\n\\n4. SYSTEM ARCHITECTURE AND TECHNOLOGY STACK\\n\\n6 https://nrf.com/research/national-retail-security-survey-2023\\n\\nScanned with\\n: 3 CamScanner’\\n'),\n",
       " Document(id='e788224c-d0a4-4dc5-be95-c46f13f1e90a', metadata={'page': 7, 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'chars': 1769, 'avg_conf': 89.67826086956522}, page_content='Incorporate computer vision\\nmodules into edge devices and loT\\nsystems.\\n\\nManufacturers\\nand integrators\\n\\nNVIDIA Jetson partners,\\nOEM Al manufacturers\\n\\nCUSTOMER ACQUISITION AND PARTNERSHIPS\\n\\n+ Identification: Market research and referrals through Lamar University\\'s\\nEntrepreneurship Center and IEEE/ACM networks.\\ne Outreach: Technical seminars, industrial conferences (AHFE, IHSI, CVPR),\\nand pilot demonstrations in university-affiliated retail environments.\\ne Partnerships:\\no Babar Inc.: Provides commercial space, kitchen equipment, and\\noperational advisory.\\no Lamar University: Offers research students for Al development,\\nmarketing, and applied learning.\\nCurry Creations LLC: Operates as a food service entity.\\nCurry Creations LLC dba EyeAl: Provides technology and analytics\\nfor automation and retail insights.\\n\\nLIST OF SERVICES\\n\\nService Description Anticipated Impact\\n\\nAl Retail Multi-camera computer vision system Reduce inventory losses\\n\\nAnalytics that detects product movements and by over 70% and improve\\ncustomer behavior (YOLOv8 + ; :\\n\\nPlatform OSNet + DeepStream). operational efficiency.\\n\\nMLOps Comprehensive development of Al Reduce implementation\\npipelines using LangChain, Airflow, ,. 6\\n\\nDeployment and MLflow for continuous model time by 60% and enable\\n\\nService retraining sustainable automation.\\n\\nRetail Advisory and training for Al Train local warkforee and\\n\\nmney : estar ela Soeees re foster regional innovation.\\n\\nStore Daily administration of store and Modernize workforce and\\n\\nintegrate technology with\\n\\nManagement _ kitchen operations. existing resources.\\n\\nService Delivery Model: Hybrid (in-person + virtual).\\n\\nPricing Structure: Customized tier plans ($25,000-$100,000 per implementation,\\ndepending on store scale).\\n\\nScanned with\\n| @ CamScanner\"\\n')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3d08e9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_doc=vectorstore.similarity_search(\"llama2 finetuning benchmark experiments.\",k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e2c1e518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='a2980a91-75a4-4ce1-bad2-04633bf1c315', metadata={'page': 10, 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'chars': 3136, 'avg_conf': 94.5107398568019}, page_content='Academically, | complemented my research work with teaching experience as a\\nGraduate Teaching Assistant at Lamar University, where | taught courses in\\nAutomated System Engineering and Computer-Aided Design and Manufacturing.\\nThis activity allowed me not only to transmit technical knowledge but also to\\ncoordinate interdisciplinary projects between engineering students and local\\nbusinesses, reinforcing my leadership and commitment to technological\\ndissemination.\\n\\nMy profile combines a solid technical foundation with strategic business vision.\\nHaving earned an MBA in Project Management from Northwestern Polytechnic\\nUniversity (California), | possess the management tools necessary to plan, finance,\\nand scale technological initiatives. This combination of engineering, data science,\\nand business management enables me to transform research into commercial\\ninnovation, and innovation into sustainable projects that strengthen the U.S.\\neconomy.\\n\\nAdditionally, | have maintained a consistent scientific output with publications in\\ninternational journals and conferences on computer vision, autonomous robotics,\\nand applied ergonomics. Among them are my articles “MultiModal Product\\nClassification Using YOLO and OCR Fusion” (2024) and “Role of Computer Vision\\nin Retail Stores” (2024), which propose technical frameworks directly applicable to\\nintelligent retail analytics. These contributions have been cited in the context of\\ncommercial automation and confirm my role as a researcher with real impact at the\\ntechnological frontier.\\n\\nThroughout my career, | have received recognitions such as the Research\\nExcellence Fellowship (2021), awarded by Lamar University for academic merit\\nand innovative potential. | also hold professional certifications that strengthen my\\ntechnical profile, including SolidWorks Professional, Software Design Assurance\\n(IEC 62304), and Certified Scrum Master (CSM), which validate my competencies\\nin both complex system design and agile management of technological projects.\\n\\nMy advanced programming expertise in Python, TensorFlow, PyTorch, OpenCV,\\nFAISS, and YOLO architectures enables me to design, train, and deploy intelligent\\nreal-time visual analysis systems. This technical proficiency, combined with my\\nunderstanding of industrial workflows and experience in automated manufacturing,\\npositions me ideally to develop applied technologies that optimize retail operations,\\nreduce losses due to theft or error, and promote a more sustainable and\\ncompetitive commercial ecosystem within the United States.\\n\\nAs part of this proposal, | lead the creation of a fast-food and retail ecosystem\\ndriven by Al, in collaboration with EyeAl and Curry Creations LLC. Our prototype\\ncafé, ready for launch, integrates computer vision, customer behavior analysis, and\\nautonomous payments, demonstrating how intelligent automation can be\\nimplemented at minimal cost through strategic academic-industrial alliances. This\\ninitiative will not only strengthen American technological competitiveness but also\\ngenerate local high-specialization jobs, foster Al adoption in small and medium-\\n\\nScanned with\\n: 3 CamScanner’\\n')]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e4a671",
   "metadata": {},
   "source": [
    "you can explore about keyword filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9e4e2674",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "df7f2e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='a2980a91-75a4-4ce1-bad2-04633bf1c315', metadata={'page': 10, 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'chars': 3136, 'avg_conf': 94.5107398568019}, page_content='Academically, | complemented my research work with teaching experience as a\\nGraduate Teaching Assistant at Lamar University, where | taught courses in\\nAutomated System Engineering and Computer-Aided Design and Manufacturing.\\nThis activity allowed me not only to transmit technical knowledge but also to\\ncoordinate interdisciplinary projects between engineering students and local\\nbusinesses, reinforcing my leadership and commitment to technological\\ndissemination.\\n\\nMy profile combines a solid technical foundation with strategic business vision.\\nHaving earned an MBA in Project Management from Northwestern Polytechnic\\nUniversity (California), | possess the management tools necessary to plan, finance,\\nand scale technological initiatives. This combination of engineering, data science,\\nand business management enables me to transform research into commercial\\ninnovation, and innovation into sustainable projects that strengthen the U.S.\\neconomy.\\n\\nAdditionally, | have maintained a consistent scientific output with publications in\\ninternational journals and conferences on computer vision, autonomous robotics,\\nand applied ergonomics. Among them are my articles “MultiModal Product\\nClassification Using YOLO and OCR Fusion” (2024) and “Role of Computer Vision\\nin Retail Stores” (2024), which propose technical frameworks directly applicable to\\nintelligent retail analytics. These contributions have been cited in the context of\\ncommercial automation and confirm my role as a researcher with real impact at the\\ntechnological frontier.\\n\\nThroughout my career, | have received recognitions such as the Research\\nExcellence Fellowship (2021), awarded by Lamar University for academic merit\\nand innovative potential. | also hold professional certifications that strengthen my\\ntechnical profile, including SolidWorks Professional, Software Design Assurance\\n(IEC 62304), and Certified Scrum Master (CSM), which validate my competencies\\nin both complex system design and agile management of technological projects.\\n\\nMy advanced programming expertise in Python, TensorFlow, PyTorch, OpenCV,\\nFAISS, and YOLO architectures enables me to design, train, and deploy intelligent\\nreal-time visual analysis systems. This technical proficiency, combined with my\\nunderstanding of industrial workflows and experience in automated manufacturing,\\npositions me ideally to develop applied technologies that optimize retail operations,\\nreduce losses due to theft or error, and promote a more sustainable and\\ncompetitive commercial ecosystem within the United States.\\n\\nAs part of this proposal, | lead the creation of a fast-food and retail ecosystem\\ndriven by Al, in collaboration with EyeAl and Curry Creations LLC. Our prototype\\ncafé, ready for launch, integrates computer vision, customer behavior analysis, and\\nautonomous payments, demonstrating how intelligent automation can be\\nimplemented at minimal cost through strategic academic-industrial alliances. This\\ninitiative will not only strengthen American technological competitiveness but also\\ngenerate local high-specialization jobs, foster Al adoption in small and medium-\\n\\nScanned with\\n: 3 CamScanner’\\n'),\n",
       " Document(id='64aa8eb8-8f0b-4c7f-b3cd-d796e0726d9a', metadata={'page': 9, 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'chars': 2543, 'avg_conf': 93.0932944606414}, page_content='aa commercial\\net a $80,000 $2,000,000 000,000 Espn ment\\n\\nPROFESSIONAL DEVELOPMENT AND KNOWLEDGE DISSEMINATION\\n\\ne Conferences: AHFE, CVPR, IEEE Computer Vision workshops, and Al\\nsummits for the retail sector.\\n\\n« Research Initiatives: Collaborative projects with Lamar University and\\nindustrial partners in retail automation and sensor fusion.\\n\\n¢ Training Programs: Mentorship for graduate students and workshops on Al\\nretail analytics and MLOps.\\n\\nCONCLUSION\\n\\nWith over a decade of combined experience in academic and industrial settings, |\\npossess a robust and practical foundation to lead the development,\\nimplementation, and commercialization of advanced retail analytics and automation\\nsystems based on artificial intelligence within the United States. My career\\nintegrates applied research, computer vision system design, technological project\\nmanagement, and innovation leadership, uniquely qualifying me to execute this\\nproposal of national impact.\\n\\nMy doctoral training in Industrial and Systems Engineering at Lamar University\\n(2021-2024), where | researched “The Role of Computer Vision in Retail Stores”\\nunder Dr. Xinyu Liu, provided me with a strong technical-scientific framework to\\nunderstand how Al systems can transform operational efficiency in the retail sector.\\nDuring this process, | developed multimodal models combining computer vision\\nand natural language processing, achieving over 93% average accuracy in retail\\nproduct recognition. This work resulted in peer-reviewed publications and the\\ncreation of the Liquor-Grocery dataset, now used to train object detection models\\nin commercial environments. This research positions me not only as a technologist\\nbut as a generator of applied knowledge with immediate market transferability.\\n\\nIn parallel, my professional experience at Vastek Inc. (contracted by Medtronic)\\nenabled me to apply advanced engineering methodologies and software validation\\nstandards (ISO 13485, IEC 62304), implementing vision-controlled automated\\nsystems for medical production lines. This strengthened my expertise in developing\\nregulated, high-reliability products—essential competencies for building safe,\\ntraceable, and scalable retail automation systems. Similarly, at Kaleidoscope\\nInnovation and Babar Inc. LLC, | contributed to the design of commercial solutions\\nintegrating sensors, cameras, RFID, and loT, developing edge-to-cloud\\narchitectures based on FastAPI, Docker, and Kubernetes, focused on the real-\\nworld implementation of Al in retail environments.\\n\\nScanned with\\n: 3 CamScanner’\\n'),\n",
       " Document(id='b8a7ac65-c41a-43b1-9b57-9b72f748453c', metadata={'page': 6, 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'chars': 2138, 'avg_conf': 90.70656370656371}, page_content='The system operates through a structured data flow, including real-time detection\\nprocessing, action analysis, and visualization in an interactive web interface.\\n\\n5, EXPECTED RESULTS AND BENEFITS\\n\\nA. Operational Benefits: Loss prevention, improved inventory accuracy, checkout\\noptimization, and labor efficiency. B. Business Intelligence Benefits: Detailed\\ncustomer behavior analytics, product performance tracking, and planogram\\ncompliance. C. Quantitative Targets: >93% accuracy in object detection, real-time\\nprocessing at 30 FPS, and 15-25% reduction in theft-related losses. D.\\nCompetitive Advantages: Reduced implementation costs, scalability for different\\nstore sizes, and privacy-preserving design.\\n\\n6. COMMERCIALIZATION STRATEGY\\n\\nService Models: SaaS, on-premise deployment, hybrid model, and API\\nlicensing. Market Segmentation: Small and medium-sized retail businesses,\\nregional chains, and large retail chains. Revenue Projections: Progressive scaling\\nfrom pilot implementations to national expansiort. Academic-Industrial\\nCollaboration: Joint research with Lamar University and __ talent\\ndevelopment. Knowledge Dissemination: Technical seminars, open-source\\ncontributions, pilot demonstrations, and training programs.\\n\\nSTRATEGIC EXPANSION\\n\\ne Phase 1 (2025-2026): Implementation in Beaumont, TX.\\n\\ne Phase 2 (2027-2029): Scaling to university-affiliated businesses in Texas\\nand neighboring states.\\n\\ne Phase 3 (2030-2032): National deployment through technology licensing\\nand integration partnerships with major retail chains.\\n\\nTARGET AUDIENCE\\n\\nSegment Purpose / Need Beneficiaries Example\\n\\nReduce losses and inventory\\nRetail companies inaccuracies through Al-based\\nand franchises product recognition and predictive\\n\\nCenvenience stores,\\nsupermarkets, franchise\\n\\nanalytics. chains\\nUniversity- Create living labs for research and Lamar University, UT\\naffiliated training in Al-driven retail Dallas, Texas A&M\\nbusinesses commerce. Retail Ventures\\nMunicipal Demonstrate autonomous service novation hubs in\\ninnovation systems (e.g., delivery robots, smart Baaumont ahd Houston\\ndistricts points of sale).\\n\\nScanned with\\n: 3 CamScanner’\\n'),\n",
       " Document(id='15331d99-1814-45ac-9984-60f0ed61d10a', metadata={'page': 5, 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'chars': 2247, 'avg_conf': 92.49477351916376}, page_content='« Deployment: Docker containerization, Kubernetes orchestration for\\nhorizontal scaling, FastAPI for edge-to-cloud pipelines, Apache Arrow IPC\\nfor zero-copy data serialization.\\n\\n2. SPECIFIC METHODOLOGIES AND TECHNIQUES\\n\\nA. Action Recognition Algorithm The system employs a probabilistic multi-state\\nmachine with temporal consistency and spatial reasoning to detect and classify\\ncustomer actions. Key functions include:\\n\\n¢ Hand-ltem Interaction Detection: Identification of interactions between\\ndetected hands and nearby products, classifying interaction strength as\\nHIGH, MEDIUM, LOW, or NONE based on overlap history (loU).\\n\\n« Probabilistic State Machine: Tracking states such as IDLE, REACHING,\\nGRASPING, PICKED, HOLDING, PLACING, and RETURNED, based on\\noverlap metrics, object velocity, and time-in-state.\\n\\ne Product Identification: Complementary methods of spatial mapping and\\nvisual embedding similarity to assign UPC codes to detected products.\\n\\n« Browsing Detection: Classification of customer behavior as BROWSING\\nbased on stationary position, small gestures, and absence of product\\ninteraction.\\n\\nB. Multi-Camera Sensor Fusion Integration of data from multiple synchronized\\ncameras using temporal alignment and spatial calibration, providing\\ncomprehensive coverage of the retail space and occlusion handling.\\n\\nC. Real-Time Processing Pipeline Optimized for real-time performance using\\nYOLOvé8/v11 models converted to TensorRT format, Parquet metadata storage with\\nefficient indexing, parallel processing with Polars, and video rendering at 60 FPS.\\n\\n3. IMPLEMENTATION PROCESS\\nDevelopment is divided into systematic phases:\\n\\n1. Data Collection and Model Training: Installation of multi-camera systems,\\ncollection of annotated datasets, model training, and optimization.\\n\\n2. System Integration: Deployment of models on edge devices,\\nimplementation of action recognition algorithms, and development of the\\nweb application.\\n\\n3. Pilot Testing: Validation in Babar Inc. locations, parameter tuning, and\\ncollection of performance metrics.\\n\\n4. Scaling and Optimization: Containerization, Kubernetes orchestration,\\ncloud deployment, and API development for third-party integration.\\n\\n4. HOW THE SYSTEM CURRENTLY OPERATES\\n\\nScanned with\\n| @ CamScanner’\\n'),\n",
       " Document(id='e5ac10f3-f685-4e94-8c0a-00a8e19cc85f', metadata={'page': 8, 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'chars': 1252, 'avg_conf': 83.33502538071066}, page_content='ORGANIZATIONAL STRUCTURE\\n\\nfe Annual Weekly ine Year of\\nn Key R i\\nPositio Salary isis y Responsibilities Incorporation\\nSaumil Patel - R&D leadership, client\\nFounder & acquisition, model\\nPrincipal $40,000 30 architecture, and project Year 1\\nEngineer supervision.\\n$40,000-$ Model training and\\nInterns (2) / : :\\nKitchen Operator 50,000 20-40 implementation, data Year 2\\neach engineering.\\nBusiness Client relations,\\nDevelopment $50,000 40 marketing, and financing Year 3\\nManager (Intern) coordination.\\nPROJECTED INITIAL COSTS\\nDescription fen)\\nCommercial space and lab setup $30,000\\nPerimeter Al hardware (GPUs, Jetsons, $20,000\\ncameras)\\nSoftware licenses / cloud infrastructure $10,000\\nMarketing and networking events $500\\nOperational expenses and permits $9,500\\nTotal $70,000\\n\\nFunding Sources: Personal savings and private investment; equipment and\\ncommercial space provided through partnership with Babar Inc. LLC.\\n\\nFINANCIAL PROJECTIONS (5-Year Horizon)\\n: Average Total\\nYear men! Pe cee. lIRevenue per|Revenue Notes\\nP Client (USD)\\n$0 Initial phase: R&D +\\nlaunch\\nExpansion with local\\n$65,000 $130,000 bariners\\n\\n—\\n2\\n(202899 2s enn is Fal\\n3 Regional\\n(2027) Bs ince __{5s00,000 implementations\\n9 $75,000 $750,000 Scaling in Texas\\n\\nScanned with\\n© CamScanner™\\n'),\n",
       " Document(id='1a300c49-04be-42cb-8b21-6aadb36c75ab', metadata={'page': 4, 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'chars': 2537, 'avg_conf': 92.88571428571429}, page_content='The proposed retail analytics system, ActionLens Studio, is a full-stack video\\nanalysis platform built on a modern, scalable architecture that combines computer\\nvision, machine learning, and cloud computing technologies. The system\\narchitecture consists of five integrated layers:\\n\\nPresentation Layer (Frontend): Built with vanilla JavaScript (ES6+), HTML5\\nVideo API, and Canvas 2D API for real-time video playback and detection\\nvisualization. This layer provides an interactive web interface where users can\\nupload videos, view detections overlaid on video frames, navigate through tracked\\nobjects, and analyze customer actions.\\n\\nAPI Layer: Implemented using FastHTML (Python web framework) with Uvicorn\\nASGI server, providing RESTful endpoints for video upload, frame metadata\\nmanagement, action recognition triggers, and data retrieval.\\n\\nBusiness Logic Layer: Contains the core action recognition algorithms including\\nActionRecognitionService, DataframeManager (Polars), DetectionLoader, and\\nFrameMetaLoader. This layer implements the state machine logic, probabilistic\\nmodeling, and temporal reasoning that drives customer action detection.\\n\\nDomain Layer (Action Recognition): Comprises specialized modules including\\nActionRecManager, ActionRecProcessor, HandinteractionManager,\\nItemInteractionManager, BrowsingDetector, and ProbabilisticActionState. These\\ncomponents. implement sophisticated algorithms for detecting and classifying\\ncustomer actions.\\n\\nData Access Layer: Utilizes Polars for high-performance dataframe operations\\n(10-100x faster than Pandas), Apache Parquet for columnar storage, Milvus vector\\ndatabase for product embedding similarity search, and Azure Blob Storage for\\ncloud data persistence.\\n\\nCore Technology Stack:\\n\\ne Backend: Python 3.9+, FastHTML 0.4.0+, Uvicorn ASGI server, Polars\\n0.20.0+ (dataframes), PyArrow 14.0+ (columnar data), Shapely 2.0+\\n(geometric operations), NumPy/SciPy (numerical operations), Pymilvus 2.5+\\n(vector database client), Azure SDK 12.19+ (cloud storage).\\n\\n« Computer Vision Models: YOLOv8 and YOLOv11 optimized with\\nTensorRT for real-time object detection, achieving >93% accuracy in retail\\nproduct recognition. The models are deployed using edge computing\\narchitectures for low-latency inference.\\n\\ne Data Storage: Parquet (columnar format for frame ‘metadata with\\npartitioning by frame_id in chunks of 1000 for O(1) lookups), Milvus (vector\\ndatabase for product embeddings with L2 distance metric), Azure Blob\\nStorage (scalable cloud object storage).\\n\\nScanned with\\n: 3 CamScanner’\\n'),\n",
       " Document(id='0ab82607-8401-48b4-9bf6-d80dcb354572', metadata={'page': 1, 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'chars': 3069, 'avg_conf': 93.67632850241546}, page_content=\"PLAN FOR FUTURE ACTIVITIES IN THE UNITED STATES\\n\\nMy proposed endeavor is to develop, implement, and commercialize advanced\\nretail analytics systems based on computer vision and autonomous store\\ntechnologies in order toenhance operational efficiency, reduce losses, and\\nincrease profitability in the U.S. retail sector. | willleverage my extensive\\nexperience of over a decade in machine learning, MLOps, and multisensory fusion,\\nas well as my doctoral research at Lamar University, “The Role of Computer Vision\\nin Retail Stores”, and my professional work at Babar Inc. LLC and Kaleidoscope\\nInnovation, to further this initiative for the benefit of the United States. My work\\nwill support advancements in artificial intelligence and automation by creating\\ndeployable and scalable computer vision systems that transform how retail\\necosystems and small businesses adopt automation. These systems will be\\nimplemented in industry through pilot programs, edgé-to-cloud deployment\\npipelines (FastAPl + Docker + Kubernetes), and autonomous sensor fusion\\ntechnologies (CCTV + RFID + loT).\\n\\nBy introducing real-time, multi-camera Al systems for customer behavior analysis,\\nproduct recognition, and autonomous payment, | will enable small and medium-\\nsized enterprises to access affordable, sustainable, and innovative automation\\nsolutions, reducing operational costs and improving profitability. Ultimately, my\\nwork willdrive American leadership in artificial intelligence and automation,\\noptimize national technological competitiveness, and contribute to economic\\ngrowth by empowering U.S. retail businesses to thrive in an increasingly\\nautomated global economy.\\n\\nWHY MY PROPOSED ENDEAVOR IS RELEVANT TO THE UNITED STATES\\n\\nThe United States currently faces a series of structural challenges in the retail and\\ntechnology sectors that affect both its economic competitiveness and its leadership\\nin innovation. One of the most significant problems is the sustained increase in\\nretail losses due to theft, inventory shrinkage, and organized crime. According to\\nthe National Retail Federation (NRF), ‘total sector losses reached'$112.1 billion in\\n2022, up from $93.9 billion the previous year, reflecting a growing crisis that\\ndirectly impacts profitability and operational security for businesses nationwide\\n(Retail Dive, 2024). This issue affects not only large chains but also small and\\nmedium-sized enterprises, which lack access to advanced prevention and\\nmonitoring systems, jeopardizing their sustainability and ability to compete in the\\nU.S. market.\\n\\nAdding to this context is the slowdown in physical retail sales, an indicator of strain\\nin the traditional economic consumption model. According to data from the U.S.\\nCensus Bureau (2024),’ total retail sales decreased by 1.6% in September 2024\\ncompared to the same month the previous year, while Trading Economics reported\\n\\n1 https://www.retaildive.com/news/retail-shrink-theft-changed-little-in-2022-nrf/694844/\\n2 https://www.census.gov/retail/mrts/www/statedata/msrs_report_september2024.pdf\\n\\nScanned with\\n: 3 CamScanner’\\n\"),\n",
       " Document(id='83338117-135c-49a9-8dc2-1a2e0a0621a2', metadata={'page': 2, 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'chars': 3084, 'avg_conf': 93.80939947780679}, page_content=\"a monthly contraction of 0.9% in January 2025.° These figures highlight stagnation\\nin physical consumption, forcing U.S. stores to transform through the adoption of\\ntechnologies that optimize operations and enhance customer experience. Without\\neffective technological transition, the loss of competitiveness in physical commerce\\ncould amplify, impacting employment and local economies across multiple regions.\\n\\nSimultaneously, the country is at a decisive moment regarding its global leadership\\nin artificial intelligence and automation. The White House recognized this need in\\nExecutive Order 14179: “Removing Barriers to American Leadership in Artificial\\nIntelligence” (January 2025), ‘stating that maintaining and strengthening American\\ndominance in Al is a strategic matter for economic prosperity and national security,\\nIn this framework, the development of applied technologies—such as computer\\nvision, machine learning, and retail automation—is considered essential to sustain\\nthe United States' competitive advantage against emerging powers. The executive\\norder emphasizes the importance of projects that combine research, development,\\nand commercial implementation, aligning directly with the objectives of this\\nproposal.\\n\\nMoreover, U.S. small and medium-sized enterprises, which represent over 40% of\\nprivate-sector jobs, face difficulties accessing automation solutions due to high\\nimplementation costs and a lack of specialized technical personnel. ‘This\\ntechnological lag creates productivity and competitiveness gaps in local\\necosystems, limiting these businesses’ ability to sustain themselves against large\\ncorporations that adopt Al-based technologies. The lack of accessible innovation\\nfor this segment not only slows economic growth but also weaker's the social and\\nlabor fabric of numerous communities, especially in semi-rural or university\\nregions.\\n\\nMy initiative directly addresses these challenges by developing, implementing, and\\ncommercializing advanced retail analytics systems based on computer vision and\\nautonomous stores in the United States. Specifically, | aim to design and deploy\\nreal-time multi-camera artificial intelligence systems capable of analyzing customer\\nbehavior, recognizing products, and facilitating autonomous payment. These\\nsystems, grounded in my doctoral research at Lamar University and my\\nprofessional experience at Babar Inc. LLC and Kaleidoscope Innovation, will\\nenable loss detection, optimize purchase flows, and reduce operational costs in a\\nscalable manner. The combination of computer vision, machine learning, and\\nmultisensory fusion (CCTV + RFID + loT) will create intelligent retail environments\\nthat help reduce inventory shrinkage and increase operational efficiency,\\naddressing the economic problem of over $112 billion in annual losses. ®\\n\\n3 https://tradingeconomics.com/united-states/retail-sales/news/448020\\n“https://www.whitehouse.gov/presidential-actions/2025/01/ removing-barriers-to-american-leadership-in-\\nartificial-intelligence/\\n\\n5 https://advocacy.sba.gov/category/research/\\n\\nScanned with\\n: 3 CamScanner’\\n\"),\n",
       " Document(id='c5d9a3a9-162e-480f-8637-e2869655dcc6', metadata={'page': 3, 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'chars': 2678, 'avg_conf': 93.14971751412429}, page_content='The project also seeks to revitalize physical retail sales by offering analytical tools\\nthat optimize customer experience through behavior pattern detection, improved\\nspace layout, and payment process automation. This enhances the\\ncompetitiveness of physical stores against the rise of e-commerce, driving a more\\nefficient and sustainable hybrid model. By implementing edge-to-cloud deployment\\npipelines (FastAPI, Docker, Kubernetes) and optimized Al models (YOLOv8/v11 >\\nTensorRT), the proposal aligns with federal priorities to boost American leadership\\nin artificial intelligence, contributing to both technological innovation and the\\ncreation of advanced automation infrastructure.\\n\\nFinally, by starting with a pilot project in Beaumont, Texas, near Lamar University,\\nthis initiative will foster academic-industrial collaboration and offer accessible\\nsolutions to small and medium-sized enterprises, derhocratizing access to\\nintelligent automation. This approach drives local economic development, creates\\ntechnological jobs, and strengthens the regional innovation ecosystem, directly\\ncontributing to the national goals of sustainability, competitiveness, and leadership\\nin artificial intelligence established by the U.S. government.\\n\\nSTEPS TO IMPLEMENT MY ENDEAVOR\\n\\n| will operate as an engineer and founder of CurryCreations, a technology-focused\\ncompany specializing in advanced retail analytics systems, computer vision,\\nartificial intelligence, and autonomous store management solutions. The company\\noperates under the brand name EyeAl Solutions for its technology and _analytics\\nservices. EyeAl Solutions LLC is headquartered in Beaumont, Texas, a strategic\\nlocation near Lamar University that provides access to technical .talent and a real-\\nworld testing ecosystem (the Babar Inc. retail cluster and Kampus Korner\\ncomplex).\\n\\nThe first testing site, a fully equipped container café and adjacent retail store, is\\nready in Beaumont, Texas, awaiting final permits within the next 15 days. This\\nspace is backed by Babar Inc., a food and retail company with over 30 years of\\nexperience in Beaumont, which provides commercial space, equipment, and\\noperational mentorship as part of a strategic alliance, reducing initial investment\\nand maximizing local expertise.\\n\\nThe venture will develop Al-driven retail automation systems through a hybrid\\nmodel of R&D collaboration and commercial implementation, focusing on edge\\ncomputing solutions and sensor fusion for autonomous retail and dining operations.\\nCOMPREHENSIVE TECHNICAL DETAIL\\n\\n4. SYSTEM ARCHITECTURE AND TECHNOLOGY STACK\\n\\n6 https://nrf.com/research/national-retail-security-survey-2023\\n\\nScanned with\\n: 3 CamScanner’\\n'),\n",
       " Document(id='e788224c-d0a4-4dc5-be95-c46f13f1e90a', metadata={'page': 7, 'source': '/home/currycreations/Desktop/New Folder/PFA-PATEL,SAUMIL.pdf', 'chars': 1769, 'avg_conf': 89.67826086956522}, page_content='Incorporate computer vision\\nmodules into edge devices and loT\\nsystems.\\n\\nManufacturers\\nand integrators\\n\\nNVIDIA Jetson partners,\\nOEM Al manufacturers\\n\\nCUSTOMER ACQUISITION AND PARTNERSHIPS\\n\\n+ Identification: Market research and referrals through Lamar University\\'s\\nEntrepreneurship Center and IEEE/ACM networks.\\ne Outreach: Technical seminars, industrial conferences (AHFE, IHSI, CVPR),\\nand pilot demonstrations in university-affiliated retail environments.\\ne Partnerships:\\no Babar Inc.: Provides commercial space, kitchen equipment, and\\noperational advisory.\\no Lamar University: Offers research students for Al development,\\nmarketing, and applied learning.\\nCurry Creations LLC: Operates as a food service entity.\\nCurry Creations LLC dba EyeAl: Provides technology and analytics\\nfor automation and retail insights.\\n\\nLIST OF SERVICES\\n\\nService Description Anticipated Impact\\n\\nAl Retail Multi-camera computer vision system Reduce inventory losses\\n\\nAnalytics that detects product movements and by over 70% and improve\\ncustomer behavior (YOLOv8 + ; :\\n\\nPlatform OSNet + DeepStream). operational efficiency.\\n\\nMLOps Comprehensive development of Al Reduce implementation\\npipelines using LangChain, Airflow, ,. 6\\n\\nDeployment and MLflow for continuous model time by 60% and enable\\n\\nService retraining sustainable automation.\\n\\nRetail Advisory and training for Al Train local warkforee and\\n\\nmney : estar ela Soeees re foster regional innovation.\\n\\nStore Daily administration of store and Modernize workforce and\\n\\nintegrate technology with\\n\\nManagement _ kitchen operations. existing resources.\\n\\nService Delivery Model: Hybrid (in-person + virtual).\\n\\nPricing Structure: Customized tier plans ($25,000-$100,000 per implementation,\\ndepending on store scale).\\n\\nScanned with\\n| @ CamScanner\"\\n')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"llama2 finetuning benchmark experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b6ba94",
   "metadata": {},
   "source": [
    "## Question: user question\n",
    "## Context: based on the question retrieving the info from the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d33ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "        Answer the question based on the context provided below.\n",
    "        If the context does not contain sufficient information, respond with:\n",
    "        \"I do not have enough information about this.\"\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a96846c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PromptTemplate from langchain_core to avoid langchain->langchain_core compatibility issues\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4f7b24f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1916f42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\n        Answer the question based on the context provided below. \\n        If the context does not contain sufficient information, respond with: \\n        \"I do not have enough information about this.\"\\n\\n        Context: {context}\\n\\n        Question: {question}\\n\\n        Answer:')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "189d799a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5bb8bad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a3efb3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "05a8c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d93729ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a7c381e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_217437/3794688567.py:5: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs_for_query = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Prompt to LLM ---\n",
      "\n",
      "        Answer the question based on the context provided below. \n",
      "        If the context does not contain sufficient information, respond with: \n",
      "        \"I do not have enough information about this.\"\n",
      "\n",
      "        Context: ORGANIZATIONAL STRUCTURE\n",
      "\n",
      "fe Annual Weekly ine Year of\n",
      "n Key R i\n",
      "Positio Salary isis y Responsibilities Incorporation\n",
      "Saumil Patel - R&D leadership, client\n",
      "Founder & acquisition, model\n",
      "Principal $40,000 30 architecture, and project Year 1\n",
      "Engineer supervision.\n",
      "$40,000-$ Model training and\n",
      "Interns (2) / : :\n",
      "Kitchen Operator 50,000 20-40 implementation, data Year 2\n",
      "each engineering.\n",
      "Business Client relations,\n",
      "Development $50,000 40 marketing, and financing Year 3\n",
      "Manager (Intern) coordination.\n",
      "PROJECTED INITIAL COSTS\n",
      "Description fen)\n",
      "Commercial space and lab setup $30,000\n",
      "Perimeter Al hardware (GPUs, Jetsons, $20,000\n",
      "cameras)\n",
      "Software licenses / cloud infrastructure $10,000\n",
      "Marketing and networking events $500\n",
      "Operational expenses and permits $9,500\n",
      "Total $70,000\n",
      "\n",
      "Funding \n",
      "--- LLM response ---\n",
      "**Saumil Patel’s Business & Technology Plan (EyeAl Solutions / Curry Creations)**  \n",
      "\n",
      "| Area | Key Points |\n",
      "|------|------------|\n",
      "| **Vision & Mission** | Deploy affordable, AI‑driven retail analytics that reduce shrinkage, improve inventory accuracy, and enable autonomous checkout for small‑to‑medium retailers. |\n",
      "| **Core Technology** | • Multi‑camera computer‑vision stack (YOLOv8/v11 → TensorRT) <br>• Edge‑to‑cloud pipeline (FastAPI, Docker, Kubernetes) <br>• Sensor fusion (CCTV + RFID + IoT) <br>• Action‑recognition state machine for customer behavior & product interaction |\n",
      "| **Product Offerings** | 1. **Retail Analytics Platform** – real‑time detection, theft‑prevention, planogram compliance. <br>2. **MLOps Suite** – automated training, deployment, and monitoring with LangChain, Airflow, MLflow. <br>3. **Retail Advisory & Training** – on‑site workshops, workforce upskilling. |\n",
      "| **Business Model** | SaaS subscription, on‑premise/hybrid deployment, API licensing. Tiered pricing: $25k–$100k per store, scaling with store size. |\n",
      "| **Go‑to‑Market Strategy** | • Pilot in Beaumont, TX (Babar Inc. retail cluster). <br>• University‑affiliated “living labs” at Lamar University for data collection & validation. <br>• Expand regionally (2027–2029) to university‑affiliated and neighboring state stores. <br>• National rollout (2030–2032) via technology licensing & partnerships with large chains. |\n",
      "| **Financial Snapshot** | • **Initial Costs**: $70k (space, hardware, software, marketing). <br>• **Revenue Projections** (5‑yr): <br> • 2025: $65k (pilot) <br> • 2026: $130k (local expansion) <br> • 2027: $250k (regional) <br> • 2028: $500k (regional) <br> • 2029: $750k (scaling in Texas) <br>• **Target**: >$1M annual recurring revenue by 2032. |\n",
      "| **Key Milestones** | 1. **Phase 1 (2025‑2026)** – Deploy pilot in Beaumont; validate 93% detection accuracy, 30 FPS real‑time. <br>2. **Phase 2 (2027‑2029)** – Scale to 50+ university‑affiliated and regional stores; refine sensor fusion & API. <br>3. **Phase 3 (2030‑2032)** – National deployment; establish licensing agreements with major chains. |\n",
      "| **Partnerships & Ecosystem** | • **Babar Inc. LLC** – commercial space, kitchen equipment, operational mentorship. <br>• **Lamar University** – research collaboration, student talent pipeline, data collection. <br>• **Kaleidoscope Innovation** – edge‑to‑cloud architecture expertise. |\n",
      "| **Impact & Alignment with U.S. Goals** | • Reduces annual retail shrinkage ($112 B+ losses). <br>• Supports White House Executive Order 14179 on AI leadership. <br>• Provides SMEs with low‑cost automation, boosting local jobs and competitiveness. |\n",
      "\n",
      "In short, Saumil Patel’s plan is to build a scalable, AI‑powered retail analytics platform, launch it through a university‑industry partnership, and progressively expand from a local pilot to national adoption, while keeping costs low for small‑to‑medium retailers and contributing to U.S. economic competitiveness.\n",
      "--- Parsed output ---\n",
      "**Saumil Patel’s Business & Technology Plan (EyeAl Solutions / Curry Creations)**  \n",
      "\n",
      "| Area | Key Points |\n",
      "|------|------------|\n",
      "| **Vision & Mission** | Deploy affordable, AI‑driven retail analytics that reduce shrinkage, improve inventory accuracy, and enable autonomous checkout for small‑to‑medium retailers. |\n",
      "| **Core Technology** | • Multi‑camera computer‑vision stack (YOLOv8/v11 → TensorRT) <br>• Edge‑to‑cloud pipeline (FastAPI, Docker, Kubernetes) <br>• Sensor fusion (CCTV + RFID + IoT) <br>• Action‑recognition state machine for customer behavior & product interaction |\n",
      "| **Product Offerings** | 1. **Retail Analytics Platform** – real‑time detection, theft‑prevention, planogram compliance. <br>2. **MLOps Suite** – automated training, deployment, and monitoring with LangChain, Airflow, MLflow. <br>3. **Retail Advisory & Training** – on‑site workshops, workforce upskilling. |\n",
      "| **Business Model** | SaaS subscription, on‑premise/hybrid deployment, API licensing. Tiered pricing: $25k–$100k per store, scaling with store size. |\n",
      "| **Go‑to‑Market Strategy** | • Pilot in Beaumont, TX (Babar Inc. retail cluster). <br>• University‑affiliated “living labs” at Lamar University for data collection & validation. <br>• Expand regionally (2027–2029) to university‑affiliated and neighboring state stores. <br>• National rollout (2030–2032) via technology licensing & partnerships with large chains. |\n",
      "| **Financial Snapshot** | • **Initial Costs**: $70k (space, hardware, software, marketing). <br>• **Revenue Projections** (5‑yr): <br> • 2025: $65k (pilot) <br> • 2026: $130k (local expansion) <br> • 2027: $250k (regional) <br> • 2028: $500k (regional) <br> • 2029: $750k (scaling in Texas) <br>• **Target**: >$1M annual recurring revenue by 2032. |\n",
      "| **Key Milestones** | 1. **Phase 1 (2025‑2026)** – Deploy pilot in Beaumont; validate 93% detection accuracy, 30 FPS real‑time. <br>2. **Phase 2 (2027‑2029)** – Scale to 50+ university‑affiliated and regional stores; refine sensor fusion & API. <br>3. **Phase 3 (2030‑2032)** – National deployment; establish licensing agreements with major chains. |\n",
      "| **Partnerships & Ecosystem** | • **Babar Inc. LLC** – commercial space, kitchen equipment, operational mentorship. <br>• **Lamar University** – research collaboration, student talent pipeline, data collection. <br>• **Kaleidoscope Innovation** – edge‑to‑cloud architecture expertise. |\n",
      "| **Impact & Alignment with U.S. Goals** | • Reduces annual retail shrinkage ($112 B+ losses). <br>• Supports White House Executive Order 14179 on AI leadership. <br>• Provides SMEs with low‑cost automation, boosting local jobs and competitiveness. |\n",
      "\n",
      "In short, Saumil Patel’s plan is to build a scalable, AI‑powered retail analytics platform, launch it through a university‑industry partnership, and progressively expand from a local pilot to national adoption, while keeping costs low for small‑to‑medium retailers and contributing to U.S. economic competitiveness.\n",
      "--- LLM response ---\n",
      "**Saumil Patel’s Business & Technology Plan (EyeAl Solutions / Curry Creations)**  \n",
      "\n",
      "| Area | Key Points |\n",
      "|------|------------|\n",
      "| **Vision & Mission** | Deploy affordable, AI‑driven retail analytics that reduce shrinkage, improve inventory accuracy, and enable autonomous checkout for small‑to‑medium retailers. |\n",
      "| **Core Technology** | • Multi‑camera computer‑vision stack (YOLOv8/v11 → TensorRT) <br>• Edge‑to‑cloud pipeline (FastAPI, Docker, Kubernetes) <br>• Sensor fusion (CCTV + RFID + IoT) <br>• Action‑recognition state machine for customer behavior & product interaction |\n",
      "| **Product Offerings** | 1. **Retail Analytics Platform** – real‑time detection, theft‑prevention, planogram compliance. <br>2. **MLOps Suite** – automated training, deployment, and monitoring with LangChain, Airflow, MLflow. <br>3. **Retail Advisory & Training** – on‑site workshops, workforce upskilling. |\n",
      "| **Business Model** | SaaS subscription, on‑premise/hybrid deployment, API licensing. Tiered pricing: $25k–$100k per store, scaling with store size. |\n",
      "| **Go‑to‑Market Strategy** | • Pilot in Beaumont, TX (Babar Inc. retail cluster). <br>• University‑affiliated “living labs” at Lamar University for data collection & validation. <br>• Expand regionally (2027–2029) to university‑affiliated and neighboring state stores. <br>• National rollout (2030–2032) via technology licensing & partnerships with large chains. |\n",
      "| **Financial Snapshot** | • **Initial Costs**: $70k (space, hardware, software, marketing). <br>• **Revenue Projections** (5‑yr): <br> • 2025: $65k (pilot) <br> • 2026: $130k (local expansion) <br> • 2027: $250k (regional) <br> • 2028: $500k (regional) <br> • 2029: $750k (scaling in Texas) <br>• **Target**: >$1M annual recurring revenue by 2032. |\n",
      "| **Key Milestones** | 1. **Phase 1 (2025‑2026)** – Deploy pilot in Beaumont; validate 93% detection accuracy, 30 FPS real‑time. <br>2. **Phase 2 (2027‑2029)** – Scale to 50+ university‑affiliated and regional stores; refine sensor fusion & API. <br>3. **Phase 3 (2030‑2032)** – National deployment; establish licensing agreements with major chains. |\n",
      "| **Partnerships & Ecosystem** | • **Babar Inc. LLC** – commercial space, kitchen equipment, operational mentorship. <br>• **Lamar University** – research collaboration, student talent pipeline, data collection. <br>• **Kaleidoscope Innovation** – edge‑to‑cloud architecture expertise. |\n",
      "| **Impact & Alignment with U.S. Goals** | • Reduces annual retail shrinkage ($112 B+ losses). <br>• Supports White House Executive Order 14179 on AI leadership. <br>• Provides SMEs with low‑cost automation, boosting local jobs and competitiveness. |\n",
      "\n",
      "In short, Saumil Patel’s plan is to build a scalable, AI‑powered retail analytics platform, launch it through a university‑industry partnership, and progressively expand from a local pilot to national adoption, while keeping costs low for small‑to‑medium retailers and contributing to U.S. economic competitiveness.\n",
      "--- Parsed output ---\n",
      "**Saumil Patel’s Business & Technology Plan (EyeAl Solutions / Curry Creations)**  \n",
      "\n",
      "| Area | Key Points |\n",
      "|------|------------|\n",
      "| **Vision & Mission** | Deploy affordable, AI‑driven retail analytics that reduce shrinkage, improve inventory accuracy, and enable autonomous checkout for small‑to‑medium retailers. |\n",
      "| **Core Technology** | • Multi‑camera computer‑vision stack (YOLOv8/v11 → TensorRT) <br>• Edge‑to‑cloud pipeline (FastAPI, Docker, Kubernetes) <br>• Sensor fusion (CCTV + RFID + IoT) <br>• Action‑recognition state machine for customer behavior & product interaction |\n",
      "| **Product Offerings** | 1. **Retail Analytics Platform** – real‑time detection, theft‑prevention, planogram compliance. <br>2. **MLOps Suite** – automated training, deployment, and monitoring with LangChain, Airflow, MLflow. <br>3. **Retail Advisory & Training** – on‑site workshops, workforce upskilling. |\n",
      "| **Business Model** | SaaS subscription, on‑premise/hybrid deployment, API licensing. Tiered pricing: $25k–$100k per store, scaling with store size. |\n",
      "| **Go‑to‑Market Strategy** | • Pilot in Beaumont, TX (Babar Inc. retail cluster). <br>• University‑affiliated “living labs” at Lamar University for data collection & validation. <br>• Expand regionally (2027–2029) to university‑affiliated and neighboring state stores. <br>• National rollout (2030–2032) via technology licensing & partnerships with large chains. |\n",
      "| **Financial Snapshot** | • **Initial Costs**: $70k (space, hardware, software, marketing). <br>• **Revenue Projections** (5‑yr): <br> • 2025: $65k (pilot) <br> • 2026: $130k (local expansion) <br> • 2027: $250k (regional) <br> • 2028: $500k (regional) <br> • 2029: $750k (scaling in Texas) <br>• **Target**: >$1M annual recurring revenue by 2032. |\n",
      "| **Key Milestones** | 1. **Phase 1 (2025‑2026)** – Deploy pilot in Beaumont; validate 93% detection accuracy, 30 FPS real‑time. <br>2. **Phase 2 (2027‑2029)** – Scale to 50+ university‑affiliated and regional stores; refine sensor fusion & API. <br>3. **Phase 3 (2030‑2032)** – National deployment; establish licensing agreements with major chains. |\n",
      "| **Partnerships & Ecosystem** | • **Babar Inc. LLC** – commercial space, kitchen equipment, operational mentorship. <br>• **Lamar University** – research collaboration, student talent pipeline, data collection. <br>• **Kaleidoscope Innovation** – edge‑to‑cloud architecture expertise. |\n",
      "| **Impact & Alignment with U.S. Goals** | • Reduces annual retail shrinkage ($112 B+ losses). <br>• Supports White House Executive Order 14179 on AI leadership. <br>• Provides SMEs with low‑cost automation, boosting local jobs and competitiveness. |\n",
      "\n",
      "In short, Saumil Patel’s plan is to build a scalable, AI‑powered retail analytics platform, launch it through a university‑industry partnership, and progressively expand from a local pilot to national adoption, while keeping costs low for small‑to‑medium retailers and contributing to U.S. economic competitiveness.\n"
     ]
    }
   ],
   "source": [
    "# Explicit retrieval -> format -> prompt -> LLM -> parse flow (avoids Runnable pipeline dependency)\n",
    "query = \"tell me about the saumil's plan?\"\n",
    "# retrieve relevant documents using the retriever object\n",
    "try:\n",
    "    docs_for_query = retriever.get_relevant_documents(query)\n",
    "except Exception:\n",
    "    # some retriever implementations use get_relevant_documents, others use retrieve; try both\n",
    "    try:\n",
    "        docs_for_query = retriever.retrieve(query)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError('Retriever does not support get_relevant_documents or retrieve: ' + str(e))\n",
    "\n",
    "context_text = format_docs(docs_for_query)\n",
    "input_text = prompt.format(context=context_text, question=query)\n",
    "print('--- Prompt to LLM ---')\n",
    "print(input_text[:1000])\n",
    "resp = llm.invoke(input_text).content\n",
    "print('--- LLM response ---')\n",
    "print(resp)\n",
    "# parse output if a parser is available\n",
    "try:\n",
    "    parsed = parser.parse(resp)\n",
    "    print('--- Parsed output ---')\n",
    "    print(parsed)\n",
    "except Exception:\n",
    "    # parser may not be necessary; ignore if missing\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b5d47b",
   "metadata": {},
   "source": [
    "# One Small task\n",
    "## Take 10 pdfs keep it in same directory and create RAG on top of it.\n",
    "### In next class will discuss about the(will start with the modular coding)\n",
    "\n",
    "exception module\n",
    "logger module\n",
    "doc analyser\n",
    "doc compare\n",
    "utils and config\n",
    "2 class\n",
    "\n",
    "2 more class api and other module\n",
    "\n",
    "2 more class for deployment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "34d26252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am saumil\\nmy name is Saumil\\ni am saumil patel'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\\n\".join([\"i am saumil\",\n",
    " \"my name is Saumil\",\n",
    " \"i am saumil patel\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f55742e",
   "metadata": {},
   "source": [
    "one specific sessin for all these topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a6ab07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
